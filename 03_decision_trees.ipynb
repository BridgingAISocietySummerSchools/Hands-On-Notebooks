{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Class 2: Decision Trees & Random Forests\n",
    "\n",
    "Welcome back! In our first class, you learned how computers find patterns using linear regression and gradient descent. Today we'll explore a completely different approach that thinks more like humans do - making decisions step by step.\n",
    "\n",
    "### ğŸ¯ What You'll Learn\n",
    "1. ğŸŒ² **Decision Trees** - How computers make decisions like humans\n",
    "2. ğŸ§  **Interactive Tree Building** - Build your own decision tree\n",
    "3. ğŸŒ²ğŸŒ²ğŸŒ² **Random Forests** - Why many trees are better than one\n",
    "4. âœ‰ï¸ **Real Applications** - Email spam detection in action\n",
    "\n",
    "### ğŸŒ³ **The Big Idea**\n",
    "\n",
    "Linear regression tries to draw a single line through data. Decision trees take a more human approach: they ask yes/no questions like:\n",
    "- \"Is the house bigger than 1,500 sq ft?\"\n",
    "- \"Does the email contain the word 'FREE'?\"\n",
    "- \"Is the patient's temperature above 38Â°C?\"\n",
    "\n",
    "This method is:\n",
    "- âœ… Interpretable (you can follow every decision),\n",
    "- âŒ Assumption-free (no need to assume linearity),\n",
    "- ğŸ”€ Flexible (works with numbers, categories, and missing data),\n",
    "- ğŸ§  Intuitive (mimics how we reason about choices).\n",
    "\n",
    "### ğŸ”— **Building on Class 1**\n",
    "- ğŸ“ˆ **Linear Regression**: Found the best line through data\n",
    "- ğŸŒ³ **Decision Trees**: Ask the best questions about data\n",
    "- ğŸ¤ **Both**: Make predictions, but in very different ways\n",
    "\n",
    "Letâ€™s explore how a tree of decisions can help a machine learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Setup - Import Our Tools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducible results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Decision Trees - Thinking Like Humans\n",
    "\n",
    "### ğŸ¤” **How Do You Make Decisions?**\n",
    "\n",
    "Imagine you're deciding whether to go outside:\n",
    "1. **Is it raining?** â†’ If YES: Stay inside â˜”\n",
    "2. **If NO: Is it sunny?** â†’ If YES: Go outside ğŸ˜\n",
    "3. **If NO: Is it too cold?** â†’ If YES: Stay inside â„ï¸, If NO: Go outside ğŸŒ¤ï¸\n",
    "\n",
    "Thatâ€™s basically how a decision tree works: a series of yes/no questions that lead to a final decision.\n",
    "\n",
    "### ğŸŒ³ **From Human Logic to Machine Learning**\n",
    "\n",
    "**Decision trees** are machine learning algorithms that:\n",
    "- **Ask questions** about the data (like \"Is age > 30?\")\n",
    "- **Split the data** based on answers\n",
    "- **Repeat** until they can make good predictions\n",
    "- **Create a tree structure** of decisions\n",
    "\n",
    "### ğŸ¯ **Why Decision Trees Are Special**\n",
    "- **Interpretable**: You can see exactly how decisions are made\n",
    "- **No assumptions**: Don't assume linear relationships like regression\n",
    "- **Handle mixed data**: Numbers, categories, missing values\n",
    "- **Natural**: Mirror human decision-making processes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ•Let's Start Simple: Predicting if Someone Likes Pizza\n",
    "\n",
    "### Generating a Realistic Dataset\n",
    "\n",
    "Let's create a simple dataset to predict if someone likes pizza based on their age, whether they like cheese, whether they are vegeatrian, and whether they have a pet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples):\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "\n",
    "    # --- Core features ---\n",
    "    ages = np.random.randint(5, 70, size=n_samples)\n",
    "    likes_cheese = np.random.binomial(1, 0.75, size=n_samples)      # Most like cheese\n",
    "    vegetarian = np.random.binomial(1, 0.3, size=n_samples)         # Minority vegetarian\n",
    "    has_pet = np.random.binomial(1, 0.5, size=n_samples)\n",
    "\n",
    "    # --- Extra features ---\n",
    "    num_siblings = np.random.poisson(1.5, size=n_samples)           # Adds noise & pattern\n",
    "    favorite_topping = np.random.choice(['pepperoni', 'mushroom', 'pineapple'], size=n_samples)\n",
    "\n",
    "    # One-hot encode topping\n",
    "    topping_dummies = pd.get_dummies(favorite_topping, prefix='topping')\n",
    "\n",
    "    # --- Calculate probability of liking pizza ---\n",
    "    prob = np.zeros(n_samples)\n",
    "\n",
    "    # Age-based base probability\n",
    "    prob += np.select(\n",
    "        [\n",
    "            ages < 18,\n",
    "            (ages >= 18) & (ages < 30),\n",
    "            (ages >= 30) & (ages < 50),\n",
    "            ages >= 50\n",
    "        ],\n",
    "        [0.70, 0.60, 0.50, 0.40]\n",
    "    )\n",
    "\n",
    "    # Add/subtract effects\n",
    "    prob += 0.15 * likes_cheese\n",
    "    prob -= 0.12 * vegetarian\n",
    "    prob += 0.10 * (likes_cheese & (favorite_topping == 'mushroom'))   # mushroom-lovers\n",
    "    prob -= 0.08 * (favorite_topping == 'pineapple')                  # ğŸ controversy!\n",
    "    prob += 0.05 * (num_siblings >= 2)\n",
    "    prob -= 0.05 * ((ages > 45) & (vegetarian == 1))                  # older vegetarians\n",
    "\n",
    "    # Add interaction bonus\n",
    "    interaction = ((ages < 25) & (vegetarian == 1) & (likes_cheese == 1)) * 0.15\n",
    "    prob += interaction\n",
    "\n",
    "    # Add some random noise\n",
    "    prob += np.random.normal(0, 0.05, size=n_samples)\n",
    "\n",
    "    # Clip to valid probability range\n",
    "    prob = np.clip(prob, 0, 1)\n",
    "\n",
    "    # Final target variable\n",
    "    likes_pizza = np.random.binomial(1, prob)\n",
    "\n",
    "    # --- Build DataFrame ---\n",
    "    pizza_data = pd.DataFrame({\n",
    "        'age': ages,\n",
    "        'likes_cheese': likes_cheese,\n",
    "        'vegetarian': vegetarian,\n",
    "        'has_pet': has_pet,\n",
    "        'num_siblings': num_siblings,\n",
    "        'topping': favorite_topping,\n",
    "    })\n",
    "\n",
    "    # Add one-hot toppings\n",
    "    pizza_data = pd.concat([pizza_data, topping_dummies], axis=1)\n",
    "    feature_cols = ['age', 'likes_cheese', 'vegetarian', 'has_pet', 'num_siblings'] + \\\n",
    "                    [col for col in pizza_data.columns if col.startswith('topping_')]\n",
    "\n",
    "    # Add target variable\n",
    "    pizza_data['likes_pizza'] = likes_pizza\n",
    "\n",
    "    return pizza_data, feature_cols\n",
    "\n",
    "# Generate the dataset\n",
    "pizza_data, feature_cols = generate_data(n_samples=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a reasonably realistic dataset â€” perfect for learning how decision trees behave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Visualizing the Dataset\n",
    "\n",
    "Letâ€™s forget how we generated it and just look at the data.\n",
    "\n",
    "(Values: 1 = yes, 0 = no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pizza_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the relationships between features and preferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pizza_data,\n",
    "                    x='age', y='likes_cheese', z='vegetarian',\n",
    "                    color='likes_pizza',\n",
    "                    color_discrete_map={0: 'red', 1: 'green'},\n",
    "                    title=\"ğŸ• Pizza Preferences in 3D\",\n",
    "                    labels={'likes_pizza': 'Likes Pizza'})\n",
    "\n",
    "fig.update_layout(height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See any patterns? A decision tree will find the best questions to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª Building Our First Decision Tree\n",
    "\n",
    "Letâ€™s build a model to predict if someone likes pizza based on what we know:\n",
    "- ğŸ§“ Age\n",
    "- ğŸ§€ Likes cheese\n",
    "- ğŸ¥— Vegetarian\n",
    "- ğŸ¶ Has a pet\n",
    "\n",
    "Weâ€™ll use `scikit-learn` to train a Decision Tree Classifier that automatically finds the best questions to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = pizza_data[feature_cols]\n",
    "y = pizza_data['likes_pizza']\n",
    "\n",
    "# Create and train a decision tree\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = tree.predict(X)\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "\n",
    "print(\"ğŸŒ³ Decision Tree Results:\")\n",
    "print(f\"   Training Accuracy: {accuracy:.1%}\")\n",
    "print(f\"   Tree Depth: {tree.get_depth()}\")\n",
    "print(f\"   Number of Leaves: {tree.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ” **What do these numbers mean?**\n",
    "\n",
    "- **Training Accuracy**: How well the model predicts on the data it was trained on. A high number might look good, but beware of overfitting!\n",
    "- **Tree Depth**: How many levels of questions the model asks before making a decision.\n",
    "- **Leaves**: The final decisions (like â€œYes, likes pizzaâ€) â€” each leaf is a possible outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”® Making a Prediction\n",
    "\n",
    "Letâ€™s use the trained tree to predict whether a 25-year-old, who:\n",
    "- ğŸ§€ Likes cheese: âœ… Yes\n",
    "- ğŸ¥¦ Vegetarian: âœ… Yes\n",
    "- ğŸ¶ Has a pet: âœ… Yes\n",
    "- ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Number of siblings: 2\n",
    "- ğŸ„ Favourite topping: Mushroom âœ…\n",
    "\n",
    "likes pizza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {\n",
    "    'age': [25],\n",
    "    'likes_cheese': [1],\n",
    "    'vegetarian': [1],\n",
    "    'has_pet': [1],\n",
    "    'num_siblings': [2],\n",
    "    'topping_mushroom': [1],\n",
    "    'topping_pepperoni': [0],\n",
    "    'topping_pineapple': [0]\n",
    "}\n",
    "new_person = pd.DataFrame(new_data)\n",
    "prediction = tree.predict(new_person)[0]\n",
    "probability = tree.predict_proba(new_person)[0]\n",
    "\n",
    "print(f\"\\nğŸ¯ Prediction for new person:\")\n",
    "print(f\"   Prediction: {'Likes Pizza! ğŸ•' if prediction == 1 else 'DoesnÂ´t like pizza ğŸ˜'}\")\n",
    "print(f\"   Confidence: {max(probability):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… This lets us peek inside the decision treeâ€™s brain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Which Features Matter?\n",
    "\n",
    "Some features have more influence than others. Letâ€™s measure **feature importance** â€” this tells us which features helped the tree make its decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': tree.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nğŸ“Š Most Important Features:\")\n",
    "for _, row in feature_importance.iterrows():\n",
    "    print(f\"   {row['feature']}: {row['importance']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice something surprising: Some features you didn't expect to be important actually are, and others you thought would matter donâ€™t show up at all. What's going on?\n",
    "\n",
    "ğŸ§  **Why This Happens**\n",
    "\n",
    "Two key effects are at play:\n",
    "\n",
    "1. **Spurious Patterns**\n",
    "\n",
    "- With small or noisy datasets, a few random coincidences can look important.\n",
    "- Example: Maybe a few pet owners in the training data liked pizza â€” the tree picks up on this, even if itâ€™s not meaningful.\n",
    "\n",
    "2. **Regularization Effects**\n",
    "\n",
    "- When we limit the treeâ€™s depth or require a minimum number of samples to split, the tree may stop early and skip less useful features.\n",
    "- This is good! It prevents the model from overfitting, but it also means that some genuinely relevant features might not show up unless they're clearly better than others.\n",
    "\n",
    "ğŸ” **What You Can Do**\n",
    "\n",
    "To handle this gracefully:\n",
    "\n",
    "- ğŸ§ª **Train/Test Split or Cross-Validation**\n",
    "\n",
    "    Always evaluate your model on unseen data â€” this tells you if a feature is truly helpful.\n",
    "\n",
    "- âœ‚ï¸ **Use Regularization Intentionally**\n",
    "\n",
    "    Adjust max_depth or min_samples_split to prevent the tree from chasing random patterns.\n",
    "\n",
    "- ğŸ” **Try Ensembles**\n",
    "\n",
    "    Random forests reduce this kind of variance by averaging over many trees (coming up next!).\n",
    "\n",
    "- ğŸ” **Feature Importance â‰  Causality**\n",
    "\n",
    "    Just because a feature is used doesn't mean it causes the outcome. Be skeptical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ® Interactive: Build Your Own Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_custom_tree(max_depth, min_samples_leaf):\n",
    "    \"\"\"Interactive tree building tool with accuracy gauge and feature importance\"\"\"\n",
    "\n",
    "    # Create tree\n",
    "    custom_tree = DecisionTreeClassifier(\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    custom_tree.fit(X, y)\n",
    "    predictions = custom_tree.predict(X)\n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "\n",
    "    importance = custom_tree.feature_importances_\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        specs=[[{\"type\": \"indicator\"}, {\"type\": \"bar\"}]],\n",
    "        subplot_titles=[\"ğŸ¯ Accuracy Gauge\", \"ğŸ“Š Feature Importance\"]\n",
    "    )\n",
    "\n",
    "    fig.add_trace(go.Indicator(\n",
    "        mode=\"gauge+number\",\n",
    "        value=accuracy * 100,\n",
    "        number={'suffix': \"%\"},\n",
    "        gauge={\n",
    "            'axis': {'range': [0, 100]},\n",
    "            'bar': {'color': \"green\"},\n",
    "            'steps': [\n",
    "                {'range': [0, 60], 'color': \"#ff4d4d\"},\n",
    "                {'range': [60, 80], 'color': \"#ffa64d\"},\n",
    "                {'range': [80, 95], 'color': \"#d4f542\"},\n",
    "                {'range': [95, 100], 'color': \"#4dff88\"}\n",
    "            ],\n",
    "        }\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    # Bar chart for feature importance\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=X.columns,\n",
    "        y=importance,\n",
    "        marker_color='teal',\n",
    "        name='Feature Importance'\n",
    "    ), row=1, col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=450,\n",
    "        title_text=f\"ğŸŒ³ Custom Decision Tree (Depth: {custom_tree.get_depth()}, Leaves: {custom_tree.get_n_leaves()})\"\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    if accuracy > 0.9:\n",
    "        print(\"\\nğŸ‰ Excellent accuracy! But be careful of overfitting...\")\n",
    "    elif accuracy > 0.75:\n",
    "        print(\"\\nâœ… Good performance!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ Try adjusting parameters for better performance\")\n",
    "\n",
    "    return custom_tree\n",
    "\n",
    "tree = build_custom_tree(max_depth=2, min_samples_leaf=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which decisions has the tree made? How does it decide if someone likes pizza? Let's visualize the decision tree and see how it splits the data based on features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plot_tree(tree,\n",
    "          feature_names=feature_cols,\n",
    "          class_names=['Dislike', 'Like'],\n",
    "          filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ‚ï¸ Splitting the Data: Train vs. Test\n",
    "\n",
    "Before we build even more powerful models, letâ€™s take a step back.\n",
    "\n",
    "Until now, weâ€™ve been evaluating models on the same data they were trained on. Thatâ€™s like studying the answers to a test and then using the same test to prove youâ€™re a genius. Not very convincing. ğŸ¤“\n",
    "\n",
    "ğŸ§ª **The Idea: Train-Test Split**\n",
    "\n",
    "To test if a model generalizes to new, unseen data, we split our dataset:\n",
    "- Training set â€“ Used to train the model\n",
    "- Test set â€“ Used to evaluate the modelâ€™s performance\n",
    "\n",
    "This lets us simulate how the model will behave in the real world!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first generate a slightly larger dataset to ensure we have enough data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_data_large, _ = generate_data(n_samples=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X_large = pizza_data_large[feature_cols]\n",
    "y_large = pizza_data_large['likes_pizza']\n",
    "\n",
    "# Split into 80% training, 20% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_large, y_large, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training size: {len(X_train)} samples\")\n",
    "print(f\"Test size:     {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸŒ³ **Training a Tree on the Training Set**\n",
    "\n",
    "Letâ€™s retrain our decision tree, but now only on the training data, and then check how well it performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=10, min_samples_leaf=10, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "train_acc = accuracy_score(y_train, tree.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, tree.predict(X_test))\n",
    "\n",
    "print(\"ğŸŒ³ Decision Tree Performance:\")\n",
    "print(f\"   Training Accuracy: {train_acc:.1%}\")\n",
    "print(f\"   Test Accuracy:     {test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually looks pretty good! The training accuracy is high, and the test accuracy is also decent. This means our model is generalizing well to new data. What if we relax some of the regularization settings?\n",
    "\n",
    "âš ï¸ **Why This Matters**\n",
    "\n",
    "- A very high training accuracy but low test accuracy means overfitting.\n",
    "- A model that performs well on the test set is more likely to work in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Random Forests - ğŸŒ²ğŸŒ³ğŸŒ² The Power of Many Trees\n",
    "\n",
    "ğŸ¤” **Why Isnâ€™t One Tree Enough?**\n",
    "\n",
    "Imagine asking one person for directions vs. asking 100 people:\n",
    "- One person might lead you astray (hello, overfitting ğŸ‘€)\n",
    "- But 100 people voting? Youâ€™re much more likely to find the right path ğŸš¶â€â™‚ï¸â¡ï¸ğŸ—ºï¸\n",
    "\n",
    "Thatâ€™s the idea behind **Random Forests**:\n",
    "- Build lots of decision trees (often 100+)\n",
    "- Each tree sees a different slice of the data\n",
    "- They vote together on the final prediction\n",
    "- The result? A **more accurate** and **more robust** model than any single tree\n",
    "\n",
    "ğŸ§ª **The Random Forest Recipe**\n",
    "\n",
    "1. Bootstrap Sampling: Each tree trains on a random subset of the data\n",
    "2. Feature Randomness: At every split, each tree considers only a random subset of features\n",
    "3. Majority Vote: For classification, the most common answer wins\n",
    "4. Outcome: A strong, stable predictor that generalizes well and resists overfitting\n",
    "\n",
    "ğŸ² **Why Is It Called Random?**\n",
    "\n",
    "Because randomness is the secret sauce:\n",
    "- âœ… Random data for each tree\n",
    "- âœ… Random features for each split\n",
    "- âœ… Random mistakes, which get averaged out\n",
    "\n",
    "ğŸ¯ This randomness helps reduce overfitting and boosts performance on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "forest_train_acc = accuracy_score(y_train, forest.predict(X_train))\n",
    "forest_test_acc = accuracy_score(y_test, forest.predict(X_test))\n",
    "\n",
    "print(\"ğŸ” Model Comparison:\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(\"ğŸŒ³ Decision Tree:\")\n",
    "print(f\"   âœ… Training Accuracy: {train_acc:.1%}\")\n",
    "print(f\"   ğŸ§ª Test Accuracy:     {test_acc:.1%}\")\n",
    "print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(\"ğŸŒ² Random Forest:\")\n",
    "print(f\"   âœ… Training Accuracy: {forest_train_acc:.1%}\")\n",
    "print(f\"   ğŸ§ª Test Accuracy:     {forest_test_acc:.1%}\")\n",
    "print(f\"   ğŸŒ¿ Number of Trees:   {len(forest.estimators_)}\")\n",
    "\n",
    "# Compare Feature Importances\n",
    "tree_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Tree Importance': tree.feature_importances_,\n",
    "    'Forest Importance': forest.feature_importances_\n",
    "}).sort_values(by='Forest Importance', ascending=False)\n",
    "\n",
    "print(\"\\nğŸ“Š Feature Importances Comparison:\")\n",
    "for _, row in tree_importance.iterrows():\n",
    "    print(f\"   {row['Feature']:<18} | Tree: {row['Tree Importance']:.3f} | Forest: {row['Forest Importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§  Why is the Random Forest More Powerful?\n",
    "\n",
    "A single decision tree is prone to **overfitting** â€” it tries to perfectly split the training data, which can make it sensitive to noise or quirks in the dataset. This often leads to high training accuracy but lower test performance.\n",
    "\n",
    "A **random forest**, on the other hand, builds many decision trees on **random subsets** of the data and features. This randomness helps:\n",
    "- ğŸ” Reduce correlation between the trees  \n",
    "- ğŸ›¡ï¸ Prevent any single feature or sample from dominating  \n",
    "\n",
    "By **averaging the predictions** of many diverse trees, the forest creates a more **stable and generalizable model**. Thatâ€™s why:\n",
    "- The forest may underfit slightly on the training set  \n",
    "- But it often **performs better on unseen data** â€” just like we saw above!\n",
    "\n",
    "ğŸ“Œ **Key Insight**: Random forests trade a bit of bias for a big reduction in variance â€” leading to better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“¦ Feature Importance: One Tree vs Many Trees\n",
    "\n",
    "In a single decision tree, the **feature importance** scores reflect how much each feature contributed to splitting the data. But in a **random forest**, each tree might make different decisions â€” especially if randomness is involved in both data and feature selection.\n",
    "\n",
    "To understand the **stability and variability** of these decisions, letâ€™s look at the feature importances **across all trees** in the forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_importances = np.array([\n",
    "    tree.feature_importances_ for tree in forest.estimators_\n",
    "])\n",
    "\n",
    "# Create box plots for each feature\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, feature in enumerate(X.columns):\n",
    "    fig.add_trace(go.Box(\n",
    "        y=all_importances[:, i],\n",
    "        name=feature,\n",
    "        boxmean='sd',\n",
    "        marker_color='teal'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"ğŸ“Š Feature Importance Across Trees (Random Forest)\",\n",
    "    yaxis_title=\"Feature Importance\",\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” What This Tells Us\n",
    "\n",
    "Each box shows the **distribution of importance values** for a given feature across all trees in the random forest. \n",
    "\n",
    "- ğŸ“ˆ Some features (like `age`) are consistently important â€” they show up in many trees with high influence.  \n",
    "- ğŸŒ€ Others vary more or are barely used â€” they may only matter in a few trees.\n",
    "\n",
    "This highlights one of the strengths of random forests: by combining diverse trees, the model captures a **broader range of signals** without relying too heavily on any single decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Back to the Tree: Why Does It Overfit?\n",
    "\n",
    "We just saw how random forests stabilize predictions by combining many shallow, varied trees. But what about a **single decision tree**?\n",
    "\n",
    "Letâ€™s investigate how the **tree depth** â€” the number of decision levels â€” affects performance. Deeper trees can make more specific decisions, but at what cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = range(1, 15)\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for d in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_accs.append(accuracy_score(y_train, model.predict(X_train)))\n",
    "    test_accs.append(accuracy_score(y_test, model.predict(X_test)))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=list(depths), y=train_accs, mode='lines+markers', name='Train Accuracy'))\n",
    "fig.add_trace(go.Scatter(x=list(depths), y=test_accs, mode='lines+markers', name='Test Accuracy'))\n",
    "fig.update_layout(title=\"Effect of Tree Depth on Accuracy\", xaxis_title=\"Max Depth\", yaxis_title=\"Accuracy\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“‰ What We See\n",
    "\n",
    "- ğŸŒ³ As depth increases, the tree becomes better at fitting the training data â€” even memorizing it.  \n",
    "- ğŸ§ª But the test accuracy suffers beyond a certain point â€” a clear sign of **overfitting**.\n",
    "- âš–ï¸ The best depth balances learning useful patterns without chasing every quirk in the data.\n",
    "\n",
    "ğŸ“Œ **Key Insight**: Individual trees can easily overfit â€” thatâ€™s why ensembles like random forests are so effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Part 3: Enter Gradient Boosting: A New Strategy\n",
    "\n",
    "Weâ€™ve seen how individual decision trees can overfit, and how random forests reduce variance by averaging many trees. But thereâ€™s another powerful idea: **Gradient Boosting**.\n",
    "\n",
    "Instead of training all trees independently (like in a forest), boosting builds them **sequentially**:\n",
    "- Each tree tries to **fix the mistakes** of the one before it.\n",
    "- The model gradually **improves**, learning from its own errors.\n",
    "- The result: a strong learner made from many weak learners. ğŸ’ª\n",
    "\n",
    "Letâ€™s compare all three approaches:\n",
    "- ğŸŒ³ A single decision tree  \n",
    "- ğŸŒ² A random forest  \n",
    "- ğŸš€ A gradient boosting machine (BDT)\n",
    "\n",
    "Which one performs best on our pizza prediction task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Decision Tree ğŸŒ³\": DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "    \"Random Forest ğŸŒ²\": RandomForestClassifier(max_depth=3, n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting ğŸš€\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    results[name] = acc\n",
    "    print(f\"\\n{name} Accuracy: {acc:.1%}\")\n",
    "    print(classification_report(y_test, preds))\n",
    "\n",
    "# Visualization\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=list(results.keys()), y=list(results.values()),\n",
    "                     text=[f\"{v:.1%}\" for v in results.values()],\n",
    "                     textposition='auto', marker_color=[\"green\", \"blue\", \"orange\"]))\n",
    "\n",
    "fig.update_layout(title=\"ğŸ“Š Model Comparison on Pizza Preference\",\n",
    "                  yaxis_title=\"Accuracy\", xaxis_title=\"Model\",\n",
    "                  height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ Results & Reflections\n",
    "\n",
    "All three models use decision trees at their core â€” but their strategies differ:\n",
    "\n",
    "- ğŸŒ³ A single tree can overfit, especially if itâ€™s deep.\n",
    "- ğŸŒ² A random forest is more stable and generalizes better by averaging many shallow trees.\n",
    "- ğŸš€ Gradient boosting (BDT) focuses on **learning from errors**, often achieving the best accuracy â€” especially on **structured tabular data**.\n",
    "\n",
    "ğŸ“Œ **Takeaway**: When accuracy matters and training time is acceptable, gradient boosting is a top choice.  \n",
    "But when speed and interpretability are more important, simpler trees or random forests still shine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”„ Boosting Step-by-Step: How Does Performance Evolve?\n",
    "\n",
    "Gradient boosting builds the model gradually, **one tree at a time**, each trying to correct the mistakes of the previous ones.\n",
    "\n",
    "But how do the **training and testing errors change** as we add more trees?\n",
    "\n",
    "Letâ€™s plot the error rate after each boosting round to see how the model improves â€” or possibly starts to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate staged errors\n",
    "bdt = models[\"Gradient Boosting ğŸš€\"]\n",
    "\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for y_train_pred, y_test_pred in zip(\n",
    "        bdt.staged_predict(X_train),\n",
    "        bdt.staged_predict(X_test)):\n",
    "    train_errors.append(1 - accuracy_score(y_train, y_train_pred))\n",
    "    test_errors.append(1 - accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Plot using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    y=train_errors,\n",
    "    mode='lines+markers',\n",
    "    name='Train Error',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    y=test_errors,\n",
    "    mode='lines+markers',\n",
    "    name='Test Error',\n",
    "    line=dict(color='red')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"ğŸ“‰ BDT Performance over Boosting Rounds\",\n",
    "    xaxis_title=\"Boosting Round\",\n",
    "    yaxis_title=\"Error Rate\",\n",
    "    height=400,\n",
    "    legend=dict(x=0.7, y=0.95)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ˆ What We Learn\n",
    "\n",
    "- ğŸŸ¦ The **training error** keeps dropping â€” the model fits the data better and better.\n",
    "- ğŸŸ¥ The **testing error** improves at first, but then may level off or even rise â€” a sign of **overfitting** if we go too far.\n",
    "- âš–ï¸ The sweet spot is usually **before the last round**, where generalization is best.\n",
    "\n",
    "ğŸ“Œ **Tip**: You can control this with **early stopping**, which halts training when test performance no longer improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ What You've Accomplished Today!\n",
    "\n",
    "In under an hour, you've explored a powerful family of machine learning algorithms built on trees â€” and compared their strengths head-to-head:\n",
    "\n",
    "### âœ… **Core Concepts Learned:**\n",
    "1. **Decision Trees** ğŸŒ³ â€“ Ask questions to make predictions\n",
    "2. **Random Forests** ğŸŒ² â€“ Combine many trees to reduce overfitting  \n",
    "3. **Gradient Boosting** ğŸš€ â€“ Learn from mistakes step-by-step\n",
    "4. **Feature Importance** ğŸ” â€“ Understand what your model really uses\n",
    "5. **Model Comparison** ğŸ“Š â€“ Evaluate accuracy and generalization\n",
    "\n",
    "### ğŸ¯ **Key Insights:**\n",
    "- **Trees think like humans** â€“ breaking decisions into simple questions\n",
    "- **Forests generalize better** â€“ by averaging many imperfect models\n",
    "- **Boosting learns iteratively** â€“ fixing errors as it goes\n",
    "- **Overfitting is real** â€“ but you can control it with depth and ensembling\n",
    "- **Machine learning is experimental** â€“ you compare, tweak, and iterate\n",
    "\n",
    "### ğŸ”— **Connecting the Classes:**\n",
    "- **Class 1 (Linear Regression)**: Parametric and interpretable  \n",
    "- **Class 2 (Trees & Boosting)**: Flexible, powerful, still interpretable  \n",
    "- **Next Class**: Neural networks and deep learning\n",
    "\n",
    "### ğŸŒŸ **The Big Picture:**\n",
    "You now understand **two core pillars** of machine learning:\n",
    "- **Linear Models** â€“ fast, elegant, and mathematically grounded  \n",
    "- **Tree-Based Models** â€“ intuitive, powerful, and great for tabular data\n",
    "\n",
    "Next up: Weâ€™ll dive into **neural networks**, the engine behind modern AI. But every step youâ€™ve taken so far gives you the right tools â€” and mindset â€” to keep climbing! ğŸ§ ğŸ”¥\n",
    "\n",
    "**Well done today!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDV-ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
