{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Class 2: Decision Trees & Random Forests\n",
    "\n",
    "Welcome back! In our first class, you learned how computers find patterns using linear regression and gradient descent. Today we'll explore a completely different approach that thinks more like humans do - making decisions step by step.\n",
    "\n",
    "### üéØ What You'll Learn\n",
    "1. üå≤ **Decision Trees** - How computers make decisions like humans\n",
    "2. üß† **Interactive Tree Building** - Build your own decision tree\n",
    "3. üå≤üå≤üå≤ **Random Forests** - Why many trees are better than one\n",
    "4. ‚úâÔ∏è **Real Applications** - Email spam detection in action\n",
    "\n",
    "### üå≥ **The Big Idea**\n",
    "\n",
    "Linear regression tries to draw a single line through data. Decision trees take a more human approach: they ask yes/no questions like:\n",
    "- \"Is the house bigger than 1,500 sq ft?\"\n",
    "- \"Does the email contain the word 'FREE'?\"\n",
    "- \"Is the patient's temperature above 38¬∞C?\"\n",
    "\n",
    "This method is:\n",
    "- ‚úÖ Interpretable (you can follow every decision),\n",
    "- ‚ùå Assumption-free (no need to assume linearity),\n",
    "- üîÄ Flexible (works with numbers, categories, and missing data),\n",
    "- üß† Intuitive (mimics how we reason about choices).\n",
    "\n",
    "### üîó **Building on Class 1**\n",
    "- üìà **Linear Regression**: Found the best line through data\n",
    "- üå≥ **Decision Trees**: Ask the best questions about data\n",
    "- ü§ù **Both**: Make predictions, but in very different ways\n",
    "\n",
    "Let‚Äôs explore how a tree of decisions can help a machine learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Setup - Import Our Tools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducible results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Decision Trees - Thinking Like Humans\n",
    "\n",
    "### ü§î **How Do You Make Decisions?**\n",
    "\n",
    "Imagine you're deciding whether to go outside:\n",
    "1. **Is it raining?** ‚Üí If YES: Stay inside ‚òî\n",
    "2. **If NO: Is it sunny?** ‚Üí If YES: Go outside üòé\n",
    "3. **If NO: Is it too cold?** ‚Üí If YES: Stay inside ‚ùÑÔ∏è, If NO: Go outside üå§Ô∏è\n",
    "\n",
    "That‚Äôs basically how a decision tree works: a series of yes/no questions that lead to a final decision.\n",
    "\n",
    "### üå≥ **From Human Logic to Machine Learning**\n",
    "\n",
    "**Decision trees** are machine learning algorithms that:\n",
    "- **Ask questions** about the data (like \"Is age > 30?\")\n",
    "- **Split the data** based on answers\n",
    "- **Repeat** until they can make good predictions\n",
    "- **Create a tree structure** of decisions\n",
    "\n",
    "### üéØ **Why Decision Trees Are Special**\n",
    "- **Interpretable**: You can see exactly how decisions are made\n",
    "- **No assumptions**: Don't assume linear relationships like regression\n",
    "- **Handle mixed data**: Numbers, categories, missing values\n",
    "- **Natural**: Mirror human decision-making processes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üçïLet's Start Simple: Predicting if Someone Likes Pizza\n",
    "\n",
    "### Generating a Realistic Dataset\n",
    "\n",
    "Let's create a simple dataset to predict if someone likes pizza based on their age, whether they like cheese, whether they are vegeatrian, and whether they have a pet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples):\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "\n",
    "    # --- Core features ---\n",
    "    ages = np.random.randint(5, 70, size=n_samples)\n",
    "    likes_cheese = np.random.binomial(1, 0.75, size=n_samples)      # Most like cheese\n",
    "    vegetarian = np.random.binomial(1, 0.3, size=n_samples)         # Minority vegetarian\n",
    "    has_pet = np.random.binomial(1, 0.5, size=n_samples)\n",
    "\n",
    "    # --- Extra features ---\n",
    "    num_siblings = np.random.poisson(1.5, size=n_samples)           # Adds noise & pattern\n",
    "    favorite_topping = np.random.choice(['pepperoni', 'mushroom', 'pineapple'], size=n_samples)\n",
    "\n",
    "    # One-hot encode topping\n",
    "    topping_dummies = pd.get_dummies(favorite_topping, prefix='topping')\n",
    "\n",
    "    # --- Calculate probability of liking pizza ---\n",
    "    prob = np.zeros(n_samples)\n",
    "\n",
    "    # Age-based base probability\n",
    "    prob += np.select(\n",
    "        [\n",
    "            ages < 18,\n",
    "            (ages >= 18) & (ages < 30),\n",
    "            (ages >= 30) & (ages < 50),\n",
    "            ages >= 50\n",
    "        ],\n",
    "        [0.70, 0.60, 0.50, 0.40]\n",
    "    )\n",
    "\n",
    "    # Add/subtract effects\n",
    "    prob += 0.15 * likes_cheese\n",
    "    prob -= 0.12 * vegetarian\n",
    "    prob += 0.10 * (likes_cheese & (favorite_topping == 'mushroom'))   # mushroom-lovers\n",
    "    prob -= 0.08 * (favorite_topping == 'pineapple')                  # üçç controversy!\n",
    "    prob += 0.05 * (num_siblings >= 2)\n",
    "    prob -= 0.05 * ((ages > 45) & (vegetarian == 1))                  # older vegetarians\n",
    "\n",
    "    # Add interaction bonus\n",
    "    interaction = ((ages < 25) & (vegetarian == 1) & (likes_cheese == 1)) * 0.15\n",
    "    prob += interaction\n",
    "\n",
    "    # Add some random noise\n",
    "    prob += np.random.normal(0, 0.05, size=n_samples)\n",
    "\n",
    "    # Clip to valid probability range\n",
    "    prob = np.clip(prob, 0, 1)\n",
    "\n",
    "    # Final target variable\n",
    "    likes_pizza = np.random.binomial(1, prob)\n",
    "\n",
    "    # --- Build DataFrame ---\n",
    "    pizza_data = pd.DataFrame({\n",
    "        'age': ages,\n",
    "        'likes_cheese': likes_cheese,\n",
    "        'vegetarian': vegetarian,\n",
    "        'has_pet': has_pet,\n",
    "        'num_siblings': num_siblings,\n",
    "        'topping': favorite_topping,\n",
    "    })\n",
    "\n",
    "    # Add one-hot toppings\n",
    "    pizza_data = pd.concat([pizza_data, topping_dummies], axis=1)\n",
    "    feature_cols = ['age', 'likes_cheese', 'vegetarian', 'has_pet', 'num_siblings'] + \\\n",
    "                    [col for col in pizza_data.columns if col.startswith('topping_')]\n",
    "\n",
    "    # Add target variable\n",
    "    pizza_data['likes_pizza'] = likes_pizza\n",
    "\n",
    "    return pizza_data, feature_cols\n",
    "\n",
    "# Generate the dataset\n",
    "pizza_data, feature_cols = generate_data(n_samples=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a reasonably realistic dataset ‚Äî perfect for learning how decision trees behave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Visualizing the Dataset\n",
    "\n",
    "Let‚Äôs forget how we generated it and just look at the data.\n",
    "\n",
    "(Values: 1 = yes, 0 = no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pizza_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the relationships between features and preferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pizza_data,\n",
    "                    x='age', y='likes_cheese', z='vegetarian',\n",
    "                    color='likes_pizza',\n",
    "                    color_discrete_map={0: 'red', 1: 'green'},\n",
    "                    title=\"üçï Pizza Preferences in 3D\",\n",
    "                    labels={'likes_pizza': 'Likes Pizza'})\n",
    "\n",
    "fig.update_layout(height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See any patterns? A decision tree will find the best questions to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Building Our First Decision Tree\n",
    "\n",
    "Let‚Äôs build a model to predict if someone likes pizza based on what we know:\n",
    "- üßì Age\n",
    "- üßÄ Likes cheese\n",
    "- ü•ó Vegetarian\n",
    "- üê∂ Has a pet\n",
    "\n",
    "We‚Äôll use `scikit-learn` to train a Decision Tree Classifier that automatically finds the best questions to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = pizza_data[feature_cols]\n",
    "y = pizza_data['likes_pizza']\n",
    "\n",
    "# Create and train a decision tree\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = tree.predict(X)\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "\n",
    "print(\"üå≥ Decision Tree Results:\")\n",
    "print(f\"   Training Accuracy: {accuracy:.1%}\")\n",
    "print(f\"   Tree Depth: {tree.get_depth()}\")\n",
    "print(f\"   Number of Leaves: {tree.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîç **What do these numbers mean?**\n",
    "\n",
    "- **Training Accuracy**: How well the model predicts on the data it was trained on. A high number might look good, but beware of overfitting!\n",
    "- **Tree Depth**: How many levels of questions the model asks before making a decision.\n",
    "- **Leaves**: The final decisions (like ‚ÄúYes, likes pizza‚Äù) ‚Äî each leaf is a possible outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÆ Making a Prediction\n",
    "\n",
    "Let‚Äôs use the trained tree to predict whether a 25-year-old, who:\n",
    "- üßÄ Likes cheese: ‚úÖ Yes\n",
    "- ü•¶ Vegetarian: ‚úÖ Yes\n",
    "- üê∂ Has a pet: ‚úÖ Yes\n",
    "- üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Number of siblings: 2\n",
    "- üçÑ Favourite topping: Mushroom ‚úÖ\n",
    "\n",
    "likes pizza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {\n",
    "    'age': [25],\n",
    "    'likes_cheese': [1],\n",
    "    'vegetarian': [1],\n",
    "    'has_pet': [1],\n",
    "    'num_siblings': [2],\n",
    "    'topping_mushroom': [1],\n",
    "    'topping_pepperoni': [0],\n",
    "    'topping_pineapple': [0]\n",
    "}\n",
    "new_person = pd.DataFrame(new_data)\n",
    "prediction = tree.predict(new_person)[0]\n",
    "probability = tree.predict_proba(new_person)[0]\n",
    "\n",
    "print(f\"\\nüéØ Prediction for new person:\")\n",
    "print(f\"   Prediction: {'Likes Pizza! üçï' if prediction == 1 else 'Doesn¬¥t like pizza üòû'}\")\n",
    "print(f\"   Confidence: {max(probability):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ This lets us peek inside the decision tree‚Äôs brain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Which Features Matter?\n",
    "\n",
    "Some features have more influence than others. Let‚Äôs measure **feature importance** ‚Äî this tells us which features helped the tree make its decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': tree.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä Most Important Features:\")\n",
    "for _, row in feature_importance.iterrows():\n",
    "    print(f\"   {row['feature']}: {row['importance']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice something surprising: Some features you didn't expect to be important actually are, and others you thought would matter don‚Äôt show up at all. What's going on?\n",
    "\n",
    "üß† **Why This Happens**\n",
    "\n",
    "Two key effects are at play:\n",
    "\n",
    "1. **Spurious Patterns**\n",
    "\n",
    "- With small or noisy datasets, a few random coincidences can look important.\n",
    "- Example: Maybe a few pet owners in the training data liked pizza ‚Äî the tree picks up on this, even if it‚Äôs not meaningful.\n",
    "\n",
    "2. **Regularization Effects**\n",
    "\n",
    "- When we limit the tree‚Äôs depth or require a minimum number of samples to split, the tree may stop early and skip less useful features.\n",
    "- This is good! It prevents the model from overfitting, but it also means that some genuinely relevant features might not show up unless they're clearly better than others.\n",
    "\n",
    "üîç **What You Can Do**\n",
    "\n",
    "To handle this gracefully:\n",
    "\n",
    "- üß™ **Train/Test Split or Cross-Validation**\n",
    "\n",
    "    Always evaluate your model on unseen data ‚Äî this tells you if a feature is truly helpful.\n",
    "\n",
    "- ‚úÇÔ∏è **Use Regularization Intentionally**\n",
    "\n",
    "    Adjust max_depth or min_samples_split to prevent the tree from chasing random patterns.\n",
    "\n",
    "- üîÅ **Try Ensembles**\n",
    "\n",
    "    Random forests reduce this kind of variance by averaging over many trees (coming up next!).\n",
    "\n",
    "- üîé **Feature Importance ‚â† Causality**\n",
    "\n",
    "    Just because a feature is used doesn't mean it causes the outcome. Be skeptical!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéÆ Interactive: Build Your Own Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_custom_tree(max_depth, min_samples_leaf):\n",
    "    \"\"\"Interactive tree building tool with accuracy gauge and feature importance\"\"\"\n",
    "\n",
    "    # Create tree\n",
    "    custom_tree = DecisionTreeClassifier(\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    custom_tree.fit(X, y)\n",
    "    predictions = custom_tree.predict(X)\n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "\n",
    "    importance = custom_tree.feature_importances_\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        specs=[[{\"type\": \"indicator\"}, {\"type\": \"bar\"}]],\n",
    "        subplot_titles=[\"üéØ Accuracy Gauge\", \"üìä Feature Importance\"]\n",
    "    )\n",
    "\n",
    "    fig.add_trace(go.Indicator(\n",
    "        mode=\"gauge+number\",\n",
    "        value=accuracy * 100,\n",
    "        number={'suffix': \"%\"},\n",
    "        gauge={\n",
    "            'axis': {'range': [0, 100]},\n",
    "            'bar': {'color': \"green\"},\n",
    "            'steps': [\n",
    "                {'range': [0, 60], 'color': \"#ff4d4d\"},\n",
    "                {'range': [60, 80], 'color': \"#ffa64d\"},\n",
    "                {'range': [80, 95], 'color': \"#d4f542\"},\n",
    "                {'range': [95, 100], 'color': \"#4dff88\"}\n",
    "            ],\n",
    "        }\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    # Bar chart for feature importance\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=X.columns,\n",
    "        y=importance,\n",
    "        marker_color='teal',\n",
    "        name='Feature Importance'\n",
    "    ), row=1, col=2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=450,\n",
    "        title_text=f\"üå≥ Custom Decision Tree (Depth: {custom_tree.get_depth()}, Leaves: {custom_tree.get_n_leaves()})\"\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    if accuracy > 0.9:\n",
    "        print(\"\\nüéâ Excellent accuracy! But be careful of overfitting...\")\n",
    "    elif accuracy > 0.75:\n",
    "        print(\"\\n‚úÖ Good performance!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Try adjusting parameters for better performance\")\n",
    "\n",
    "    return custom_tree\n",
    "\n",
    "tree = build_custom_tree(max_depth=2, min_samples_leaf=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which decisions has the tree made? How does it decide if someone likes pizza? Let's visualize the decision tree and see how it splits the data based on features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plot_tree(tree,\n",
    "          feature_names=feature_cols,\n",
    "          class_names=['Dislike', 'Like'],\n",
    "          filled=True, rounded=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÇÔ∏è Splitting the Data: Train vs. Test\n",
    "\n",
    "Before we build even more powerful models, let‚Äôs take a step back.\n",
    "\n",
    "Until now, we‚Äôve been evaluating models on the same data they were trained on. That‚Äôs like studying the answers to a test and then using the same test to prove you‚Äôre a genius. Not very convincing. ü§ì\n",
    "\n",
    "üß™ **The Idea: Train-Test Split**\n",
    "\n",
    "To test if a model generalizes to new, unseen data, we split our dataset:\n",
    "- Training set ‚Äì Used to train the model\n",
    "- Test set ‚Äì Used to evaluate the model‚Äôs performance\n",
    "\n",
    "This lets us simulate how the model will behave in the real world!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first generate a slightly larger dataset to ensure we have enough data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_data_large, _ = generate_data(n_samples=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X_large = pizza_data_large[feature_cols]\n",
    "y_large = pizza_data_large['likes_pizza']\n",
    "\n",
    "# Split into 80% training, 20% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_large, y_large, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training size: {len(X_train)} samples\")\n",
    "print(f\"Test size:     {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üå≥ **Training a Tree on the Training Set**\n",
    "\n",
    "Let‚Äôs retrain our decision tree, but now only on the training data, and then check how well it performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=10, min_samples_leaf=10, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "train_acc = accuracy_score(y_train, tree.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, tree.predict(X_test))\n",
    "\n",
    "print(\"üå≥ Decision Tree Performance:\")\n",
    "print(f\"   Training Accuracy: {train_acc:.1%}\")\n",
    "print(f\"   Test Accuracy:     {test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually looks pretty good! The training accuracy is high, and the test accuracy is also decent. This means our model is generalizing well to new data. What if we relax some of the regularization settings?\n",
    "\n",
    "‚ö†Ô∏è **Why This Matters**\n",
    "\n",
    "- A very high training accuracy but low test accuracy means overfitting.\n",
    "- A model that performs well on the test set is more likely to work in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Random Forests - üå≤üå≥üå≤ The Power of Many Trees\n",
    "\n",
    "ü§î **Why Isn‚Äôt One Tree Enough?**\n",
    "\n",
    "Imagine asking one person for directions vs. asking 100 people:\n",
    "- One person might lead you astray (hello, overfitting üëÄ)\n",
    "- But 100 people voting? You‚Äôre much more likely to find the right path üö∂‚Äç‚ôÇÔ∏è‚û°Ô∏èüó∫Ô∏è\n",
    "\n",
    "That‚Äôs the idea behind **Random Forests**:\n",
    "- Build lots of decision trees (often 100+)\n",
    "- Each tree sees a different slice of the data\n",
    "- They vote together on the final prediction\n",
    "- The result? A **more accurate** and **more robust** model than any single tree\n",
    "\n",
    "üß™ **The Random Forest Recipe**\n",
    "\n",
    "1. Bootstrap Sampling: Each tree trains on a random subset of the data\n",
    "2. Feature Randomness: At every split, each tree considers only a random subset of features\n",
    "3. Majority Vote: For classification, the most common answer wins\n",
    "4. Outcome: A strong, stable predictor that generalizes well and resists overfitting\n",
    "\n",
    "üé≤ **Why Is It Called Random?**\n",
    "\n",
    "Because randomness is the secret sauce:\n",
    "- ‚úÖ Random data for each tree\n",
    "- ‚úÖ Random features for each split\n",
    "- ‚úÖ Random mistakes, which get averaged out\n",
    "\n",
    "üéØ This randomness helps reduce overfitting and boosts performance on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "forest_train_acc = accuracy_score(y_train, forest.predict(X_train))\n",
    "forest_test_acc = accuracy_score(y_test, forest.predict(X_test))\n",
    "\n",
    "print(\"üîç Model Comparison:\")\n",
    "print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(\"üå≥ Decision Tree:\")\n",
    "print(f\"   ‚úÖ Training Accuracy: {train_acc:.1%}\")\n",
    "print(f\"   üß™ Test Accuracy:     {test_acc:.1%}\")\n",
    "print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(\"üå≤ Random Forest:\")\n",
    "print(f\"   ‚úÖ Training Accuracy: {forest_train_acc:.1%}\")\n",
    "print(f\"   üß™ Test Accuracy:     {forest_test_acc:.1%}\")\n",
    "print(f\"   üåø Number of Trees:   {len(forest.estimators_)}\")\n",
    "\n",
    "# Compare Feature Importances\n",
    "tree_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Tree Importance': tree.feature_importances_,\n",
    "    'Forest Importance': forest.feature_importances_\n",
    "}).sort_values(by='Forest Importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Feature Importances Comparison:\")\n",
    "for _, row in tree_importance.iterrows():\n",
    "    print(f\"   {row['Feature']:<18} | Tree: {row['Tree Importance']:.3f} | Forest: {row['Forest Importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Why is the Random Forest More Powerful?\n",
    "\n",
    "A single decision tree is prone to **overfitting** ‚Äî it tries to perfectly split the training data, which can make it sensitive to noise or quirks in the dataset. This often leads to high training accuracy but lower test performance.\n",
    "\n",
    "A **random forest**, on the other hand, builds many decision trees on **random subsets** of the data and features. This randomness helps:\n",
    "- üîÅ Reduce correlation between the trees  \n",
    "- üõ°Ô∏è Prevent any single feature or sample from dominating  \n",
    "\n",
    "By **averaging the predictions** of many diverse trees, the forest creates a more **stable and generalizable model**. That‚Äôs why:\n",
    "- The forest may underfit slightly on the training set  \n",
    "- But it often **performs better on unseen data** ‚Äî just like we saw above!\n",
    "\n",
    "üìå **Key Insight**: Random forests trade a bit of bias for a big reduction in variance ‚Äî leading to better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Feature Importance: One Tree vs Many Trees\n",
    "\n",
    "In a single decision tree, the **feature importance** scores reflect how much each feature contributed to splitting the data. But in a **random forest**, each tree might make different decisions ‚Äî especially if randomness is involved in both data and feature selection.\n",
    "\n",
    "To understand the **stability and variability** of these decisions, let‚Äôs look at the feature importances **across all trees** in the forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_importances = np.array([\n",
    "    tree.feature_importances_ for tree in forest.estimators_\n",
    "])\n",
    "\n",
    "# Create box plots for each feature\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, feature in enumerate(X.columns):\n",
    "    fig.add_trace(go.Box(\n",
    "        y=all_importances[:, i],\n",
    "        name=feature,\n",
    "        boxmean='sd',\n",
    "        marker_color='teal'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"üìä Feature Importance Across Trees (Random Forest)\",\n",
    "    yaxis_title=\"Feature Importance\",\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What This Tells Us\n",
    "\n",
    "Each box shows the **distribution of importance values** for a given feature across all trees in the random forest. \n",
    "\n",
    "- üìà Some features (like `age`) are consistently important ‚Äî they show up in many trees with high influence.  \n",
    "- üåÄ Others vary more or are barely used ‚Äî they may only matter in a few trees.\n",
    "\n",
    "This highlights one of the strengths of random forests: by combining diverse trees, the model captures a **broader range of signals** without relying too heavily on any single decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÅ Back to the Tree: Why Does It Overfit?\n",
    "\n",
    "We just saw how random forests stabilize predictions by combining many shallow, varied trees. But what about a **single decision tree**?\n",
    "\n",
    "Let‚Äôs investigate how the **tree depth** ‚Äî the number of decision levels ‚Äî affects performance. Deeper trees can make more specific decisions, but at what cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = range(1, 15)\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "for d in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_accs.append(accuracy_score(y_train, model.predict(X_train)))\n",
    "    test_accs.append(accuracy_score(y_test, model.predict(X_test)))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=list(depths), y=train_accs, mode='lines+markers', name='Train Accuracy'))\n",
    "fig.add_trace(go.Scatter(x=list(depths), y=test_accs, mode='lines+markers', name='Test Accuracy'))\n",
    "fig.update_layout(title=\"Effect of Tree Depth on Accuracy\", xaxis_title=\"Max Depth\", yaxis_title=\"Accuracy\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìâ What We See\n",
    "\n",
    "- üå≥ As depth increases, the tree becomes better at fitting the training data ‚Äî even memorizing it.  \n",
    "- üß™ But the test accuracy suffers beyond a certain point ‚Äî a clear sign of **overfitting**.\n",
    "- ‚öñÔ∏è The best depth balances learning useful patterns without chasing every quirk in the data.\n",
    "\n",
    "üìå **Key Insight**: Individual trees can easily overfit ‚Äî that‚Äôs why ensembles like random forests are so effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Part 3: Enter Gradient Boosting: A New Strategy\n",
    "\n",
    "We‚Äôve seen how individual decision trees can overfit, and how random forests reduce variance by averaging many trees. But there‚Äôs another powerful idea: **Gradient Boosting**.\n",
    "\n",
    "Instead of training all trees independently (like in a forest), boosting builds them **sequentially**:\n",
    "- Each tree tries to **fix the mistakes** of the one before it.\n",
    "- The model gradually **improves**, learning from its own errors.\n",
    "- The result: a strong learner made from many weak learners. üí™\n",
    "\n",
    "Let‚Äôs compare all three approaches:\n",
    "- üå≥ A single decision tree  \n",
    "- üå≤ A random forest  \n",
    "- üöÄ A gradient boosting machine (BDT)\n",
    "\n",
    "Which one performs best on our pizza prediction task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Decision Tree üå≥\": DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "    \"Random Forest üå≤\": RandomForestClassifier(max_depth=3, n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting üöÄ\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    results[name] = acc\n",
    "    print(f\"\\n{name} Accuracy: {acc:.1%}\")\n",
    "    print(classification_report(y_test, preds))\n",
    "\n",
    "# Visualization\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=list(results.keys()), y=list(results.values()),\n",
    "                     text=[f\"{v:.1%}\" for v in results.values()],\n",
    "                     textposition='auto', marker_color=[\"green\", \"blue\", \"orange\"]))\n",
    "\n",
    "fig.update_layout(title=\"üìä Model Comparison on Pizza Preference\",\n",
    "                  yaxis_title=\"Accuracy\", xaxis_title=\"Model\",\n",
    "                  height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÅ Results & Reflections\n",
    "\n",
    "All three models use decision trees at their core ‚Äî but their strategies differ:\n",
    "\n",
    "- üå≥ A single tree can overfit, especially if it‚Äôs deep.\n",
    "- üå≤ A random forest is more stable and generalizes better by averaging many shallow trees.\n",
    "- üöÄ Gradient boosting (BDT) focuses on **learning from errors**, often achieving the best accuracy ‚Äî especially on **structured tabular data**.\n",
    "\n",
    "üìå **Takeaway**: When accuracy matters and training time is acceptable, gradient boosting is a top choice.  \n",
    "But when speed and interpretability are more important, simpler trees or random forests still shine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Boosting Step-by-Step: How Does Performance Evolve?\n",
    "\n",
    "Gradient boosting builds the model gradually, **one tree at a time**, each trying to correct the mistakes of the previous ones.\n",
    "\n",
    "But how do the **training and testing errors change** as we add more trees?\n",
    "\n",
    "Let‚Äôs plot the error rate after each boosting round to see how the model improves ‚Äî or possibly starts to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate staged errors\n",
    "bdt = models[\"Gradient Boosting üöÄ\"]\n",
    "\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for y_train_pred, y_test_pred in zip(\n",
    "        bdt.staged_predict(X_train),\n",
    "        bdt.staged_predict(X_test)):\n",
    "    train_errors.append(1 - accuracy_score(y_train, y_train_pred))\n",
    "    test_errors.append(1 - accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Plot using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    y=train_errors,\n",
    "    mode='lines+markers',\n",
    "    name='Train Error',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    y=test_errors,\n",
    "    mode='lines+markers',\n",
    "    name='Test Error',\n",
    "    line=dict(color='red')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"üìâ BDT Performance over Boosting Rounds\",\n",
    "    xaxis_title=\"Boosting Round\",\n",
    "    yaxis_title=\"Error Rate\",\n",
    "    height=400,\n",
    "    legend=dict(x=0.7, y=0.95)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà What We Learn\n",
    "\n",
    "- üü¶ The **training error** keeps dropping ‚Äî the model fits the data better and better.\n",
    "- üü• The **testing error** improves at first, but then may level off or even rise ‚Äî a sign of **overfitting** if we go too far.\n",
    "- ‚öñÔ∏è The sweet spot is usually **before the last round**, where generalization is best.\n",
    "\n",
    "üìå **Tip**: You can control this with **early stopping**, which halts training when test performance no longer improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ What You've Accomplished Today!\n",
    "\n",
    "In under an hour, you've explored a powerful family of machine learning algorithms built on trees ‚Äî and compared their strengths head-to-head:\n",
    "\n",
    "### ‚úÖ **Core Concepts Learned:**\n",
    "1. **Decision Trees** üå≥ ‚Äì Ask questions to make predictions\n",
    "2. **Random Forests** üå≤ ‚Äì Combine many trees to reduce overfitting  \n",
    "3. **Gradient Boosting** üöÄ ‚Äì Learn from mistakes step-by-step\n",
    "4. **Feature Importance** üîç ‚Äì Understand what your model really uses\n",
    "5. **Model Comparison** üìä ‚Äì Evaluate accuracy and generalization\n",
    "\n",
    "### üéØ **Key Insights:**\n",
    "- **Trees think like humans** ‚Äì breaking decisions into simple questions\n",
    "- **Forests generalize better** ‚Äì by averaging many imperfect models\n",
    "- **Boosting learns iteratively** ‚Äì fixing errors as it goes\n",
    "- **Overfitting is real** ‚Äì but you can control it with depth and ensembling\n",
    "- **Machine learning is experimental** ‚Äì you compare, tweak, and iterate\n",
    "\n",
    "### üîó **Connecting the Classes:**\n",
    "- **Class 1 (Linear Regression)**: Parametric and interpretable  \n",
    "- **Class 2 (Trees & Boosting)**: Flexible, powerful, still interpretable  \n",
    "- **Next Class**: Neural networks and deep learning\n",
    "\n",
    "### üåü **The Big Picture:**\n",
    "You now understand **two core pillars** of machine learning:\n",
    "- **Linear Models** ‚Äì fast, elegant, and mathematically grounded  \n",
    "- **Tree-Based Models** ‚Äì intuitive, powerful, and great for tabular data\n",
    "\n",
    "Next up: We‚Äôll dive into **neural networks**, the engine behind modern AI. But every step you‚Äôve taken so far gives you the right tools ‚Äî and mindset ‚Äî to keep climbing! üß†üî•\n",
    "\n",
    "**Well done today!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDV-ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
