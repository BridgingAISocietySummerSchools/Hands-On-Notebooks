{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Machine Learning Fundamentals: Linear Regression & Gradient Descent\n",
    "\n",
    "Welcome to your first step into the world of machine learning! In this notebook, we'll explore how computers learn from data ‚Äî starting with one of the most powerful and intuitive ideas in AI: **learning by improving**.\n",
    "\n",
    "We‚Äôll break down two essential building blocks of modern machine learning:\n",
    "\n",
    "**üéØ What You'll Learn:**\n",
    "1. **Linear Regression** ‚Äì How machines find patterns and make predictions using lines  \n",
    "2. **Gradient Descent** ‚Äì How computers improve their guesses through trial and error  \n",
    "3. **Real-World Example** ‚Äì Applying these tools to a relatable scenario\n",
    "\n",
    "Whether you‚Äôre a curious beginner or brushing up your foundations, this notebook is designed to be:  \n",
    "- ‚úÖ **Visual** ‚Äì with clear plots to show what‚Äôs happening  \n",
    "- ‚úÖ **Interactive** ‚Äì so you can tweak the data and see the results  \n",
    "- ‚úÖ **Accessible** ‚Äì no advanced math required, just an open mind\n",
    "\n",
    "**üì¶ What‚Äôs Inside:**\n",
    "- A gentle introduction to core ideas  \n",
    "- A bottom-up learning path with minimal prerequisites  \n",
    "- A real-world mini project to tie it all together  \n",
    "- Code you can copy, extend, and reuse\n",
    "\n",
    "**üí° Why It Matters:**  \n",
    "These simple tools ‚Äî linear regression and gradient descent ‚Äî are the backbone of many AI systems. Understanding them gives you a clear window into how models learn from data, how optimization works, and how predictions are made.\n",
    "\n",
    "> *From predicting house prices to powering deep learning ‚Äî this is where it all begins.*\n",
    "\n",
    "---\n",
    "\n",
    "Ready? Let‚Äôs teach machines to learn! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Setup - Import Our Tools. Run this cell first (takes ~10 seconds).\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducible results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Linear Regression - Finding the Pattern\n",
    "\n",
    "### üè† The House Price Challenge\n",
    "\n",
    "Imagine you're a real estate agent. A client asks: *\"How much should I price my 1,800 sq ft house?\"* You have data from recent sales. How do you find the pattern?\n",
    "\n",
    "**Linear regression** finds the best straight line through data points - like drawing the \"line of best fit\" you might remember from school, but done automatically by a computer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating some realistic house price data. This will help us visualize how linear regression works in practice. The house sizes are in square feet, and the prices are in thousands of dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_sizes = np.array([800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2600])\n",
    "house_prices = np.array([150, 180, 220, 250, 280, 320, 350, 380, 420, 450])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† What's in this data?\n",
    "\n",
    "- Each house is described by **one feature**: its size (in square feet).\n",
    "- The **target** we're trying to predict is the **price** (in $1000s).\n",
    "- This is a typical supervised learning setup: we want to learn a rule that maps inputs to outputs.\n",
    "\n",
    "Let's print out the house sizes and prices in data pairs to see what we're working with. The first element is the input (house size), and the second is the output (price)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print out the house sizes and prices.\n",
    "print(\"üè† Recent House Sales Data:\")\n",
    "for size, price in zip(house_sizes, house_prices):\n",
    "    print(f\"   {size:,} sq ft ‚Üí ${price}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizations help us understand data better. We'll plot the house sizes against their prices. Maybe that will help us see a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=house_sizes, y=house_prices,\n",
    "    mode='markers',\n",
    "    marker=dict(size=12, color='blue'),\n",
    "    name='House Sales',\n",
    "    hovertemplate='Size: %{x:,} sq ft<br>Price: $%{y}k<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"üè† House Prices vs Size - Can You See the Pattern?\",\n",
    "    xaxis_title=\"House Size (sq ft)\",\n",
    "    yaxis_title=\"Price ($1000s)\",\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î Question: If you had to draw a straight line through these points, where would you draw it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéÆ Interactive: Try to Find the Best Line Yourself!\n",
    "\n",
    "Now it's your turn! Let's see if you can find the best line that fits the data. The following code allows you to manually adjust the slope and intercept of a line to see how well it fits the data. \n",
    "\n",
    "- üéØ Try different values to minimize the error!\n",
    "- üí° The 'best' line minimizes the average squared error.\n",
    "\n",
    "**Squared error** = the difference between the actual prices and the predicted prices from your line, squared to avoid negative values. The goal is to make this error as small as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_manual_line(slope, intercept):\n",
    "    \"\"\"Interactive tool to manually adjust the line\"\"\"\n",
    "\n",
    "    # Calculate predictions with manual line\n",
    "    predictions = slope * house_sizes + intercept\n",
    "\n",
    "    # Calculate error\n",
    "    error = np.mean((house_prices - predictions) ** 2)\n",
    "\n",
    "    # Create plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Original data\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=house_sizes, y=house_prices,\n",
    "        mode='markers',\n",
    "        marker=dict(size=12, color='blue'),\n",
    "        name='Actual Prices'\n",
    "    ))\n",
    "\n",
    "    # Manual line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=house_sizes, y=predictions,\n",
    "        mode='lines',\n",
    "        line=dict(color='red', width=3),\n",
    "        name='Your Line'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Your Line: Price = {slope:.3f} x Size + {intercept:.1f} | Error: {error:.1f}\",\n",
    "        xaxis_title=\"House Size (sq ft)\",\n",
    "        yaxis_title=\"Price ($1000s)\",\n",
    "        height=400\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    plot_manual_line,\n",
    "    slope=widgets.FloatSlider(\n",
    "        value=0.00,\n",
    "        min=0.0,\n",
    "        max=1.0,\n",
    "        step=0.01,\n",
    "        description='Slope'\n",
    "    ),\n",
    "    intercept=widgets.FloatSlider(\n",
    "        value=300,\n",
    "        min=-200,\n",
    "        max=400,\n",
    "        step=1,\n",
    "        description='Intercept'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ Now Let's See How the Computer Finds the Best Line\n",
    "\n",
    "Now let's see how the computer finds the best line automatically using linear regression. We'll use a simple linear regression model to fit the data and visualize the results. This uses the normal equation method to find the optimal slope and intercept (an analytic solution to the linear regression problem).\n",
    "\n",
    "Does the line look like the one you drew? If not, don't worry! The computer uses a systematic approach to find the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression model\n",
    "model = LinearRegression()\n",
    "X = house_sizes.reshape(-1, 1)  # Reshape for sklearn\n",
    "y = house_prices\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get the best line parameters\n",
    "best_slope = model.coef_[0]\n",
    "best_intercept = model.intercept_\n",
    "best_predictions = model.predict(X)\n",
    "best_error = mean_squared_error(y, best_predictions)\n",
    "\n",
    "# Visualize the result\n",
    "fig = go.Figure()\n",
    "\n",
    "# Original data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=house_sizes, y=house_prices,\n",
    "    mode='markers',\n",
    "    marker=dict(size=12, color='blue'),\n",
    "    name='Actual Prices'\n",
    "))\n",
    "\n",
    "# Computer's best line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=house_sizes, y=best_predictions,\n",
    "    mode='lines',\n",
    "    line=dict(color='green', width=3),\n",
    "    name='Computer\\'s Best Line'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"ü§ñ Computer's Best Line: Price = ${best_slope:.3f} x Size + ${best_intercept:.1f} (Error: {best_error:.2f})\",\n",
    "    xaxis_title=\"House Size (sq ft)\",\n",
    "    yaxis_title=\"Price ($1000)\",\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a good fit! Let's see how the computer interprets this line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"   ‚Ä¢ Each additional sq ft adds ${best_slope*1000:.0f} to the price\")\n",
    "print(f\"   ‚Ä¢ A 0 sq ft house would cost ${best_intercept*1000:.0f} (base value)\")\n",
    "print(f\"   ‚Ä¢ üéØ Estimated price an 1,800 sq ft house: ${model.predict(np.array([[1800]]))[0]:.1f}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have a model that predicts house prices based on their sizes. The computer's best line gives us a systematic way to estimate prices, and we can see how it fits the data. This is how machine learning helps us find patterns in data and make predictions based on those patterns. If you have any questions or want to explore more, feel free to ask! üòä\n",
    "\n",
    "There are some important aspects to consider when letting a computer find the model:\n",
    "- **We** still need to pick the model type (linear regression in this case). It is up to us to decide if this is the right model for our data.\n",
    "- **We** need to define exactly what the model is trying to accomplish. In this case, we want to minimize the squared error between the predicted prices and the actual prices. This is called the training objective.\n",
    "- **We** need to define exactly how the model accomplishes this objective. In this case, we find the solution by minimizing the squared error between the predicted prices and the actual prices using the normal equation method.\n",
    "- **We** need to curate the data that we use to train the model. In this case, we have a small dataset of house sizes and prices, but in practice, we would want to use a larger and more diverse dataset to ensure the model generalizes well.\n",
    "\n",
    "All in all, this is a simple example of how machine learning works. We define the model, the objective, and the data, and then let the computer find the best solution. Computers are only as smart as we make them, and they need our guidance to learn effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gradient Descent - How Computers Learn\n",
    "\n",
    "### üèîÔ∏è The Mountain Climbing Analogy\n",
    "\n",
    "The computer found the best line for our housing price problem, but **how** did it do that? Imagine you're hiking in thick fog and want to reach the bottom of a valley (the lowest error). You can't see far, but you can feel which direction slopes downward. So you:\n",
    "\n",
    "1. **Feel the ground** around your feet (measure the slope)\n",
    "2. **Take a step** downhill (adjust your position)  \n",
    "3. **Repeat** until you reach the bottom (find the minimum error)\n",
    "\n",
    "This is **gradient descent** - the fundamental algorithm that powers most machine learning!\n",
    "\n",
    "**üéØ Why This Matters:** neural networks use gradient descent to learn. They adjust their parameters (weights) iteratively to minimize the error between predicted and actual outputs. This is how deep learning models learn complex patterns in data. Learning this principle on simple examples helps us understand how more complex models work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé¨ Gradient Descent in Action - Step by Step\n",
    "\n",
    "The following code simulates the gradient descent process for our house price model. It starts with a random line and iteratively adjusts it to minimize the error.\n",
    "\n",
    "We need to do some data acrobatics to make the gradient descent work: the algorithm only works efficiently if the data is normalized (mean = 0, standard deviation = 1). This helps the algorithm converge faster and more reliably. So we need to normalize and 'denormalize' the data before and after the training process.\n",
    "\n",
    "Let's see it in action! We run the gradient descent algorithm with two parameters:\n",
    "- **Learning rate**: How big of a step we take downhill each time. A small value means we take small steps, while a larger value means we take bigger steps.\n",
    "- **Number of iterations**: How many times we repeat the process of feeling the ground and taking a step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    \"\"\"Normalize data for stable gradient descent\"\"\"\n",
    "    return (data - np.mean(data)) / np.std(data), np.mean(data), np.std(data)\n",
    "\n",
    "def denormalize_slope(norm_slope, std_x):\n",
    "    \"\"\"Convert normalized slope back to original scale\"\"\"\n",
    "    return norm_slope / std_x\n",
    "\n",
    "def denormalize_intercept(norm_slope, mean_x, std_x, mean_y):\n",
    "    \"\"\"Convert normalized intercept back to original scale\"\"\"\n",
    "    orig_slope = denormalize_slope(norm_slope, std_x)\n",
    "    return mean_y - orig_slope * mean_x\n",
    "\n",
    "# Normalize data for stable learning\n",
    "house_sizes_norm, mean_size, std_size = normalize_data(house_sizes)\n",
    "mean_price = np.mean(house_prices)\n",
    "\n",
    "def gradient_descent_demo(learning_rate, steps):\n",
    "    \"\"\"Demonstrate gradient descent step by step\"\"\"\n",
    "\n",
    "    # Start with random guess\n",
    "    slope = 0\n",
    "    intercept = 0\n",
    "    slope_orig = 0\n",
    "    intercept_orig = 0\n",
    "\n",
    "    # Track progress\n",
    "    history = {'step': [], 'slope': [], 'intercept': [], 'error': []}\n",
    "\n",
    "    print(f\"üöÄ Starting gradient descent (LR={learning_rate}, {steps} steps)\")\n",
    "    print(\"Step | Error   | Slope  | Intercept\")\n",
    "    print(\"-\" * 35)\n",
    "\n",
    "    for step in range(steps):\n",
    "        # Calculate predictions and error\n",
    "        predictions = slope * house_sizes_norm + intercept\n",
    "        error = np.mean((house_prices - predictions) ** 2)\n",
    "\n",
    "        # Convert to original scale for display\n",
    "        slope_orig = denormalize_slope(slope, std_size)\n",
    "        intercept_orig = denormalize_intercept(slope, mean_size, std_size, mean_price)\n",
    "\n",
    "        # Record progress\n",
    "        history['step'].append(step)\n",
    "        history['slope'].append(slope_orig)\n",
    "        history['intercept'].append(intercept_orig)\n",
    "        history['error'].append(error)\n",
    "\n",
    "        # Print progress every 10 steps\n",
    "        if step % 10 == 0:\n",
    "            print(f\"{step:4d} | {error:7.2f} | {slope_orig:6.4f} | {intercept_orig:9.2f}\")\n",
    "\n",
    "        # Calculate gradients (which direction to move)\n",
    "        n = len(house_sizes_norm)\n",
    "        errors = house_prices - predictions\n",
    "        slope_gradient = -2 * np.sum(errors * house_sizes_norm) / n\n",
    "        intercept_gradient = -2 * np.sum(errors) / n\n",
    "\n",
    "        # Take a step downhill\n",
    "        slope = slope - learning_rate * slope_gradient\n",
    "        intercept = intercept - learning_rate * intercept_gradient\n",
    "\n",
    "    print(f\"\\nüèÅ Final result: Slope={slope_orig:.4f}, Intercept={intercept_orig:.2f}\")\n",
    "    print(f\"üéØ Compare to computer's solution: Slope={best_slope:.4f}, Intercept={best_intercept:.2f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "# Run the demonstration\n",
    "history = gradient_descent_demo(learning_rate=0.02, steps=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see how the slope and intercept change over time, and how the error decreases as the model learns. We can also see that we aren't quite at the bottom of the valley yet, but we are getting closer with each step. How many more steps do you think it will take to reach the bottom?\n",
    "\n",
    "It's probably better to get a visualization of the gradient descent process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Visualize the Learning Process\n",
    "\n",
    "You'll 'see' the learning process in action! The following code visualizes how the model learns over time. It shows how the slope and intercept change with each step, and how the error decreases as the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_process(history):\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        specs=[[{}, {\"secondary_y\": True}]],\n",
    "        subplot_titles=('Error Decreases Over Time', 'Parameters Converge to Solution')\n",
    "    )\n",
    "\n",
    "    # Error over time\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=history['step'], y=history['error'],\n",
    "                mode='lines+markers', name='Error',\n",
    "                line=dict(color='red', width=2)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Parameters over time\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=history['step'], y=history['slope'],\n",
    "                mode='lines', name='Slope',\n",
    "                line=dict(color='blue', width=2)),\n",
    "        row=1, col=2, secondary_y=False\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=history['step'], y=history['intercept'],\n",
    "                mode='lines', name='Intercept',\n",
    "                line=dict(color='green', width=2)),\n",
    "        row=1, col=2, secondary_y=True\n",
    "    )\n",
    "\n",
    "    # Add target lines\n",
    "    fig.add_hline(y=best_slope, line_dash=\"dash\", line_color=\"blue\", row=1, col=2, secondary_y=False)  # type: ignore\n",
    "    fig.add_hline(y=best_intercept, line_dash=\"dash\", line_color=\"green\", row=1, col=2, secondary_y=True)  # type: ignore\n",
    "\n",
    "    fig.update_layout(height=400, title_text=\"üèîÔ∏è Gradient Descent: Going Downhill to Find the Solution\")\n",
    "    fig.update_xaxes(title_text=\"Step\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Step\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Error\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Slope\", row=1, col=2, secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"Intercept\", row=1, col=2, secondary_y=True)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "plot_learning_process(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç What you're seeing:**\n",
    "- Left: Error goes down as the algorithm learns (\"going downhill\")\n",
    "- Right: Parameters gradually approach the optimal values (dashed lines). The slope already looks pretty good from the start, but the intercept is still far from the optimal value. This shows how gradient descent iteratively improves the model parameters.\n",
    "\n",
    "This is exactly how neural networks train, too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéõÔ∏è Interactive: Effect of Learning Rate\n",
    "\n",
    "Let's explore how the learning rate affects the gradient descent process. The learning rate determines how big of a step we take downhill each time. A small learning rate means we take small steps, while a larger learning rate means we take bigger steps.\n",
    "\n",
    "The following code allows you to adjust the learning rate and see how it affects the gradient descent process. You can try different values and see how quickly the model converges to the optimal solution. By default, the code runs 50 iterations, but you can change this to see how the model learns over time.\n",
    "\n",
    "**üéØ Can you find a learning rate that works well?**\n",
    "\n",
    "- If the learning rate is too small, the model takes a long time to converge.\n",
    "- If the learning rate is too large, the model may overshoot the optimal solution and oscillate around it, or even diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_learning_rate(learning_rate, n_steps):\n",
    "    slope, intercept = 0.0, 0.0\n",
    "    errors = []\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        predictions = slope * house_sizes_norm + intercept\n",
    "        error = np.mean((house_prices - predictions) ** 2)\n",
    "        errors.append(error)\n",
    "\n",
    "        # Calculate gradients\n",
    "        n = len(house_sizes_norm)\n",
    "        error_diff = house_prices - predictions\n",
    "        slope_gradient = -2 * np.sum(error_diff * house_sizes_norm) / n\n",
    "        intercept_gradient = -2 * np.sum(error_diff) / n\n",
    "\n",
    "        # Update parameters\n",
    "        slope = slope - learning_rate * slope_gradient\n",
    "        intercept = intercept - learning_rate * intercept_gradient\n",
    "\n",
    "    # Plot results\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(n_steps)), y=errors,\n",
    "        mode='lines+markers',\n",
    "        name=f'LR = {learning_rate}',\n",
    "        line=dict(width=3)\n",
    "    ))\n",
    "\n",
    "    max_error = max(errors)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Learning Rate = {learning_rate}: {'‚úÖ Good' if errors[-1] < 100 else '‚ö†Ô∏è Too fast' if learning_rate > 0.1 else 'üêå Too slow'}\",\n",
    "        xaxis_title=\"Step\",\n",
    "        yaxis_title=\"Error\",\n",
    "        height=300,\n",
    "        yaxis=dict(range=[0, max(1000, max_error * 1.1)])\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    final_slope = denormalize_slope(slope, std_size)\n",
    "    final_intercept = denormalize_intercept(slope, mean_size, std_size, mean_price)\n",
    "\n",
    "    print(f\"Final: Slope={final_slope:.4f}, Intercept={final_intercept:.2f}, Error={errors[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "interact(\n",
    "    test_learning_rate,\n",
    "    learning_rate=widgets.FloatLogSlider(\n",
    "        value=0.01,\n",
    "        base=10,\n",
    "        min=-4,  # 0.0001\n",
    "        max=0,   # 1.0\n",
    "        step=0.1,\n",
    "        description='Learning Rate'\n",
    "    ),\n",
    "    n_steps=widgets.IntSlider(\n",
    "        value=70,\n",
    "        min=1,\n",
    "        max=200,\n",
    "        step=1,\n",
    "        description='Steps'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåÄ Gradient Descent, but Stochastic!\n",
    "\n",
    "The gradient descent we just saw used all data points at once to compute the slope and intercept update. This is called batch gradient descent.\n",
    "\n",
    "But what if we had millions of data points? It would take a long time to compute the gradients at every step!\n",
    "\n",
    "Stochastic Gradient Descent (SGD) speeds this up by using only one data point at a time to update the parameters. It‚Äôs faster and more memory efficient ‚Äî and often generalizes better, too.\n",
    "\n",
    "**üß† Two Ways to Learn: Batch vs Stochastic Gradient Descent**\n",
    "\n",
    "| Method                  | Uses               | Pros                            | Cons                          |\n",
    "|-------------------------|--------------------|----------------------------------|-------------------------------|\n",
    "| **Batch Gradient Descent**     | All data at each step | Stable steps, clear convergence | Can be slow for large datasets |\n",
    "| **Stochastic Gradient Descent** | One point at a time    | Fast updates, good generalization | Noisy path, less stable       |\n",
    "\n",
    "\n",
    "The following code repeats the gradient descent process, but this time using stochastic gradient descent. It updates the slope and intercept using only one data point at a time, which speeds up the learning process. How do you think it compares to the batch gradient descent we saw earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_demo(learning_rate, steps):\n",
    "    \"\"\"Stochastic Gradient Descent using one data point at a time\"\"\"\n",
    "\n",
    "    slope = 0\n",
    "    intercept = 0\n",
    "    slope_orig = 0\n",
    "    intercept_orig = 0\n",
    "\n",
    "    history = {'step': [], 'slope': [], 'intercept': [], 'error': []}\n",
    "\n",
    "    print(f\"üéØ Starting SGD (LR={learning_rate}, {steps} steps)\")\n",
    "\n",
    "    for step in range(steps):\n",
    "        # Randomly pick one data point\n",
    "        idx = np.random.randint(0, len(house_sizes_norm))\n",
    "        x_i = house_sizes_norm[idx]\n",
    "        y_i = house_prices[idx]\n",
    "\n",
    "        # Prediction for just this point\n",
    "        prediction = slope * x_i + intercept\n",
    "        error = (y_i - prediction) ** 2\n",
    "\n",
    "        # Gradients for this one point\n",
    "        slope_grad = -2 * x_i * (y_i - prediction)\n",
    "        intercept_grad = -2 * (y_i - prediction)\n",
    "\n",
    "        # Update parameters\n",
    "        slope -= learning_rate * slope_grad\n",
    "        intercept -= learning_rate * intercept_grad\n",
    "\n",
    "        # Track in original scale\n",
    "        slope_orig = denormalize_slope(slope, std_size)\n",
    "        intercept_orig = denormalize_intercept(slope, mean_size, std_size, mean_price)\n",
    "\n",
    "        history['step'].append(step)\n",
    "        history['slope'].append(slope_orig)\n",
    "        history['intercept'].append(intercept_orig)\n",
    "        history['error'].append(error)\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"{step:4d} | Error: {error:.2f} | Slope: {slope_orig:.4f} | Intercept: {intercept_orig:.2f}\")\n",
    "\n",
    "    print(f\"\\nüèÅ Final SGD result: Slope={slope_orig:.4f}, Intercept={intercept_orig:.2f}\")\n",
    "    return history\n",
    "\n",
    "sgd_history = sgd_demo(learning_rate=0.03, steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new result does not look quite as good as the one we got with batch gradient descent, but keep in mind that each step is based on only one data point. This means training with 50 steps lets the model only see 50 data points in total, while the batch gradient descent used all data points already 50 times!\n",
    "\n",
    "Let's visualize the learning process again to see how the model learns with stochastic gradient descent. The following code shows how the slope and intercept change over time, and how the error decreases as the model learns. You will notice that everything looks a bit more noisy than with batch gradient descent, but the model still learns and improves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_process(sgd_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: Quick Real-World Application (5 minutes)\n",
    "\n",
    "### üìö Predicting Data Scientist Productivity\n",
    "\n",
    "Let‚Äôs bring regression into the daily life of a data scientist ‚Äî and yes, that includes coffee.\n",
    "\n",
    "In this real-world-style example, we‚Äôll try to predict how many tasks a data scientist gets done based on how many cups of coffee they drink per day. It's a relatable scenario: some caffeine, some inspiration, maybe a bit of chaos ‚Äî but is there a measurable pattern?\n",
    "\n",
    "Our fictional dataset tracks daily coffee intake and task completion. With it, you‚Äôll:\n",
    "- Explore whether productivity increases linearly with coffee consumption\n",
    "- Train a simple linear model on one feature: cups of coffee\n",
    "- Use the model to predict performance for new caffeine levels (including dangerously high ones!)\n",
    "- Visualize the trend and ask yourself: does more always mean better?\n",
    "\n",
    "This is a playful example, but it reflects the kind of quick exploratory modeling that kicks off many real-world data projects. It‚Äôs a chance to practice everything you‚Äôve learned in a familiar but fun setting ‚Äî and who knows, maybe you‚Äôll discover your optimal coffee zone along the way. ‚òï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fictional data: Coffee intake (cups/day) vs tasks completed\n",
    "coffee_cups = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "tasks_done = np.array([2, 4, 6, 8, 10, 11, 11, 10, 9])  # Diminishing returns!\n",
    "\n",
    "print(\"‚òï Coffee and Productivity Data:\")\n",
    "for cups, tasks in zip(coffee_cups, tasks_done):\n",
    "    print(f\"   {cups} cup(s)/day ‚Üí {tasks} tasks completed\")\n",
    "\n",
    "# Train linear regression model\n",
    "coffee_model = LinearRegression()\n",
    "coffee_model.fit(coffee_cups.reshape(-1, 1), tasks_done)\n",
    "\n",
    "# Predict productivity for a given input\n",
    "cups_input = 6\n",
    "predicted_tasks = coffee_model.predict(np.array([[cups_input]]))[0]\n",
    "\n",
    "print(f\"\\nüéØ Prediction: With {cups_input} cups of coffee/day\")\n",
    "print(f\"   ‚Üí predicted productivity: {predicted_tasks:.1f} tasks/day\")\n",
    "\n",
    "# Visualization\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=coffee_cups, y=tasks_done, mode='markers',\n",
    "                         marker=dict(size=10, color='brown'), name='Observed Data'))\n",
    "\n",
    "# Add prediction line\n",
    "pred_line = coffee_model.predict(coffee_cups.reshape(-1, 1))\n",
    "fig.add_trace(go.Scatter(x=coffee_cups, y=pred_line, mode='lines',\n",
    "                         line=dict(color='darkred', width=3), name='Regression Line'))\n",
    "\n",
    "# Mark the prediction\n",
    "fig.add_trace(go.Scatter(x=[cups_input], y=[predicted_tasks], mode='markers',\n",
    "                         marker=dict(size=15, color='crimson', symbol='star'),\n",
    "                         name='Your Prediction'))\n",
    "\n",
    "fig.update_layout(title=\"‚òï Coffee Intake vs Tasks Completed\",\n",
    "                  xaxis_title=\"Cups of Coffee per Day\",\n",
    "                  yaxis_title=\"Tasks Completed per Day\",\n",
    "                  height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Insight:\n",
    "- Each extra cup of coffee adds 2 completed tasks ‚Äî up to a point!\n",
    "- The data shows a non-linear story: productivity plateaus (and may even drop!) after about 6 cups.\n",
    "- A linear model is easy to fit, but it doesn't capture diminishing returns ‚Äî or the initial boost accurately.\n",
    "\n",
    "üëâ This is a great reminder: always check your model assumptions before trusting the predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üéâ What You've Accomplished Today!\n",
    "\n",
    "In just 45 minutes, you've explored the foundations of machine learning ‚Äî the same building blocks behind modern AI systems used in everything from recommendation engines to autonomous vehicles.\n",
    "\n",
    "‚úÖ **Core Concepts Mastered:**\n",
    "1. **Linear Regression** - Using simple models to uncover patterns and make predictions\n",
    "2. **Gradient Descent** - Understanding how machines \"learn\" by iteratively minimizing errors\n",
    "3. **Real-World Framing** - Applying models to relatable problems, and learning when they break\n",
    "\n",
    "üöÄ **Key Insights:**\n",
    "- **Machine learning starts simple** ‚Äî with lines and gradients ‚Äî but these tools scale to powerful models.\n",
    "- **Model assumptions matter** ‚Äì A linear model is fast and interpretable, but it can miss the big picture.\n",
    "- **Learning is iterative** ‚Äì Algorithms improve step by step, just like we do.\n",
    "\n",
    "üéØ **Next Steps:**\n",
    "1. **Try your own data** - Apply these concepts to problems you care about\n",
    "2. **Learn more algorithms** - Decision trees, neural networks, etc.\n",
    "\n",
    "\n",
    "üåü **The Big Picture:**\n",
    "\n",
    "What you‚Äôve seen today ‚Äî especially gradient descent ‚Äî isn‚Äôt just a classroom exercise. It‚Äôs the beating heart of deep learning, which uses the same principles to train complex neural networks on massive datasets.\n",
    "\n",
    "Understanding these fundamentals gives you:\n",
    "- The confidence to explore more complex models\n",
    "- The ability to debug and demystify what‚Äôs happening under the hood\n",
    "- A clear lens on where machine learning excels ‚Äî and where caution is needed\n",
    "\n",
    "üëè **Congratulations!** You now understand the core principles that power the AI revolution. The tools may grow in complexity, but the core ideas ‚Äî patterns, learning, and optimization ‚Äî start right here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDV-ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
