{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning Class 3: Neural Networks & Deep Learning\n",
    "\n",
    "Welcome to the third and final class in our machine learning journey!\n",
    "So far, we‚Äôve learned how:\n",
    "\n",
    "- Linear models uncover simple trends in data üìà\n",
    "- Decision trees ask smart questions to classify and predict üå≥\n",
    "\n",
    "Today, we turn to the most powerful tool in modern AI:\n",
    "\n",
    "### ‚ú® Neural Networks ‚Äî systems inspired by the brain that can learn almost anything.\n",
    "\n",
    "### üéØ What You'll Learn Today\n",
    "\n",
    "1. **Neural Network Basics** ‚Äî How artificial neurons learn from data\n",
    "2. **Building Your First Network** ‚Äî Step-by-step hands-on demo\n",
    "3. **Going Deeper** ‚Äî Why ‚Äúdeep‚Äù learning is so powerful\n",
    "4. **Real Applications** ‚Äî From image recognition to language translation\n",
    "5. **Bonus** ‚Äî Learn how to build the same network in Keras and PyTorch\n",
    "\n",
    "### üß† **The Core Idea**\n",
    "\n",
    "Neural networks are made of layers of tiny computing units ‚Äî neurons ‚Äî that:\n",
    "\n",
    "- Take inputs (like pixel values or text features)\n",
    "- Apply weights and nonlinear activations\n",
    "- Pass results forward to make predictions\n",
    "\n",
    "With enough neurons and layers, these networks can:\n",
    "\n",
    "- Recognize complex images üñºÔ∏è\n",
    "- Understand language and context üåç\n",
    "- Learn intricate patterns that no human could code by hand\n",
    "\n",
    "### üîó Building on Previous Classes\n",
    "\n",
    "| Class | Focus | Key Idea |\n",
    "|-------|-------|----------|\n",
    "| 1Ô∏è‚É£ Linear Models | Fit lines to patterns | Models assume a simple structure |\n",
    "| 2Ô∏è‚É£ Trees | Ask yes/no questions | Let data dictate structure |\n",
    "| 3Ô∏è‚É£ Neural Nets | Learn anything | Model learns structure from scratch |\n",
    "\n",
    "Neural networks **don‚Äôt need hand-coded rules**. They build their own understanding by adjusting internal parameters based on data ‚Äî making them the foundation of everything from ChatGPT to self-driving cars.\n",
    "\n",
    "### üåç Why Neural Networks Matter\n",
    "\n",
    "They power many of the tools you use daily:\n",
    "\n",
    "- üì∏ Image classifiers on your phone\n",
    "- üß† Large Language Models like ChatGPT\n",
    "- üéß Music & movie recommendations\n",
    "- üöó Autonomous vehicles\n",
    "- üß¨ Healthcare diagnostics\n",
    "\n",
    "And they‚Äôre just getting started.\n",
    "\n",
    "---\n",
    "\n",
    "Let‚Äôs dive in and build one from scratch ‚Äî then see how to train it using real-world libraries like TensorFlow/Keras and PyTorch.\n",
    "\n",
    "Get ready to enter the world of deep learning! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Google Colab, clone the repo (if needed),\n",
    "# move into the repo directory, and ensure it‚Äôs on the Python path.\n",
    "\n",
    "import sys, os\n",
    "\n",
    "def in_colab():\n",
    "    try: import google.colab; return True\n",
    "    except: return False\n",
    "\n",
    "if in_colab():\n",
    "    repo = \"Hands-On-Notebooks\"\n",
    "    if os.path.basename(os.getcwd()) != repo:\n",
    "        if not os.path.exists(repo):\n",
    "            !git clone https://github.com/BridgingAISocietySummerSchools/{repo}\n",
    "        %cd {repo}\n",
    "    if '.' not in sys.path:\n",
    "        sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Setup - Import Our Neural Network Tools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_circles, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Input\n",
    "\n",
    "from plotting_utils.neural_nets import (\n",
    "    plot_customer_data_scatter,\n",
    "    create_interactive_neuron_designer,\n",
    "    plot_circle_data_scatter,\n",
    "    plot_neural_network_results,\n",
    "    plot_digit_samples,\n",
    "    create_interactive_digit_classifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducible results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Neural Networks - Artificial Brains\n",
    "\n",
    "### üß† **How Does Your Brain Work?**\n",
    "\n",
    "Your brain has ~86 billion neurons that:\n",
    "1. **Receive signals** from other neurons\n",
    "2. **Process information** by combining signals\n",
    "3. **Send output** to other neurons if activated\n",
    "4. **Learn** by strengthening important connections\n",
    "\n",
    "**Artificial neural networks** are a *simplified version* of this idea:\n",
    "\n",
    "- They use numbers instead of electric signals.\n",
    "- They \"learn\" by changing weights through data and feedback.\n",
    "\n",
    "### üîó **From Biological to Artificial**\n",
    "\n",
    "Let's break down what a single **artificial neuron** does:\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Inputs** | Numbers representing features (like age, income, etc.) |\n",
    "| **Weights** | How important each input is (learned during training) |\n",
    "| **Activation** | Decides whether to \"fire\" based on weighted inputs |\n",
    "| **Output** | A number passed to the next layer |\n",
    "\n",
    "A **neural network** simply connects many of these neurons in layers:\n",
    "\n",
    "- **Input layer** takes your data\n",
    "- **Hidden layers** do the processing\n",
    "- **Output layer** gives the final prediction\n",
    "\n",
    "### üéØ Why Networks Beat Single Neurons\n",
    "\n",
    "- A single neuron can only draw a straight line ‚Äî it‚Äôs like linear regression.\n",
    "- A network with more neurons and layers can:\n",
    "  - Bend, curve, and twist boundaries üåÄ\n",
    "  - Combine patterns from multiple inputs\n",
    "  - Capture non-linear structures in data\n",
    "\n",
    "Let‚Äôs build some intuition with a real example. üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Example: Can a Single Neuron Predict Who Will Buy?\n",
    "\n",
    "We‚Äôll use a tiny **customer dataset** with just two inputs: `age` and `income`.\n",
    "\n",
    "Each customer either bought a product (1) or did not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data = pd.DataFrame({\n",
    "    'age': [25, 35, 45, 55, 30, 40, 50, 28, 38, 48],\n",
    "    'income': [30, 50, 70, 90, 40, 60, 80, 35, 55, 75],\n",
    "    'will_buy': [0, 0, 1, 1, 0, 1, 1, 0, 0, 1]\n",
    "})\n",
    "print(customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Let's Visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_customer_data_scatter(customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Testing a Single Neuron\n",
    "\n",
    "Let‚Äôs simulate how one artificial neuron would handle a single customer. Let's assume the custom is 40 years old with an income of $60,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_neuron(age, income, weight_age=0.1, weight_income=0.05, bias=-3):\n",
    "    weighted_sum = weight_age * age + weight_income * income + bias\n",
    "\n",
    "    # Let's use a sigmoid activation function\n",
    "    activation = 1 / (1 + np.exp(-weighted_sum))\n",
    "    prediction = 1 if activation > 0.5 else 0\n",
    "\n",
    "    return prediction, activation\n",
    "\n",
    "prediction, confidence = simple_neuron(age=40, income=60)\n",
    "print(f\"Prediction: {'Will Buy' if prediction == 1 else 'Will Not Buy'}, Confidence: {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéÆ Interactive: Design Your Own Neuron\n",
    "\n",
    "The following code defines a helper function to visualize how a single artificial neuron makes predictions.  \n",
    "Don‚Äôt worry if it looks long ‚Äî it's just setting up the plotting and interaction.\n",
    "\n",
    "Now, use the sliders below to adjust the weights and bias and see how your neuron performs!  \n",
    "Can you find a setting that gets most predictions right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_interactive_neuron_designer(customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What This Does:\n",
    "\n",
    "- It shows you how well your weights perform.\n",
    "- The first plot shows which predictions were right (green) or wrong (red).\n",
    "- The second plot shows your neuron‚Äôs decision boundary: where it predicts ‚Äúyes‚Äù vs ‚Äúno‚Äù.\n",
    "\n",
    "### Interpreting the Output:\n",
    "\n",
    "- A boundary that cuts through the data well means your neuron learned something useful!\n",
    "- A boundary that misses many points means the neuron is too simple ‚Äî or poorly tuned.\n",
    "- Look at the accuracy printed ‚Äî aim for at least 80%!\n",
    "\n",
    "üß† This helps you build intuition for:\n",
    "- What weights and bias do\n",
    "- How decision boundaries work\n",
    "- Why real neural networks need more neurons and more layers\n",
    "\n",
    "### üí° Key Takeaways\n",
    "\n",
    "- A single neuron is powerful but limited.\n",
    "- It struggles with curved or complex boundaries.\n",
    "- You just discovered why we need multi-layer neural networks ‚Äî to handle complex patterns in data!\n",
    "\n",
    "üëâ Next, we‚Äôll move from one neuron to an actual network, and see how it improves performance dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building Your First Neural Network\n",
    "\n",
    "### üîÑ From a Single Neuron to a Learning Machine\n",
    "\n",
    "A single neuron can draw a line ‚Äî but it can‚Äôt capture curves, corners, or complex decision boundaries.  \n",
    "To move beyond that, we need **many neurons** working together in **layers**.\n",
    "\n",
    "This is the core idea of a **neural network**.\n",
    "\n",
    "### üß± What Happens Inside a Neural Network?\n",
    "\n",
    "- The **input layer** passes features to the network\n",
    "- **Hidden layers** process information step by step:\n",
    "  - The first layer might detect simple signals\n",
    "  - The next layer combines them into patterns\n",
    "  - Later layers build more abstract understanding\n",
    "- The **output layer** makes the final prediction\n",
    "\n",
    "### üåü Why Is This Powerful?\n",
    "\n",
    "- **Multiple neurons** allow flexible, curved decision boundaries  \n",
    "- **Multiple layers** allow abstraction: from pixels ‚Üí edges ‚Üí shapes ‚Üí faces  \n",
    "- **Activation functions** give the network its non-linear magic\n",
    "\n",
    "With the right architecture, a neural network can learn just about anything.\n",
    "\n",
    "Let‚Äôs build one!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåÄ Step 1: Create a Challenging Dataset\n",
    "\n",
    "We‚Äôll now create a dataset that **cannot be solved with straight lines**.  \n",
    "It has circular patterns ‚Äî something only neural networks can untangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex dataset that needs a neural network\n",
    "X, y = make_circles(n_samples=300, noise=0.1, factor=0.3, random_state=42)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "circle_data = pd.DataFrame(X, columns=['x', 'y'])\n",
    "circle_data['class'] = y\n",
    "\n",
    "print(\"üéØ Complex Pattern Dataset:\")\n",
    "print(f\"   Data points: {len(circle_data)}\")\n",
    "print(f\"   Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Step 2: Visualize the Problem\n",
    "\n",
    "Let‚Äôs plot the data to see why this is tricky.  \n",
    "The two classes form **concentric circles** ‚Äî a single line won‚Äôt separate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_circle_data_scatter(circle_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† **Observation**:  \n",
    "- ‚ùå **Linear models** will fail here  \n",
    "- ‚ùå **Single neurons** won‚Äôt help  \n",
    "- ‚úÖ **Neural networks** are up for the challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è Step 3: Prepare the Data\n",
    "\n",
    "We‚Äôll now split the data and scale it.  \n",
    "**Why scale?** Neural networks are sensitive to the scale of input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the data (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ Step 4: Build and Train a Neural Network\n",
    "\n",
    "Let‚Äôs create a real neural network with:\n",
    "- **2 hidden layers**\n",
    "- **10 neurons each**\n",
    "- **ReLU activation**\n",
    "- Up to **1000 training iterations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network with Keras\n",
    "model = keras.Sequential([\n",
    "    Input(shape=(2,)),\n",
    "    layers.Dense(10, activation='relu'),\n",
    "    layers.Dense(10, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model and store training history\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, verbose=0, validation_data=(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Step 5: Evaluate Performance\n",
    "\n",
    "Now let‚Äôs test the network‚Äôs accuracy and see how well it learned the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training and test sets\n",
    "train_loss, train_accuracy = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "# Report results\n",
    "print(f\"\\nüß† Neural Network Results:\")\n",
    "print(f\"   Training Accuracy: {train_accuracy:.1%}\")\n",
    "print(f\"   Test Accuracy: {test_accuracy:.1%}\")\n",
    "print(f\"   Network Architecture: Input ‚Üí 10 ‚Üí 10 ‚Üí Output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ Great job! You've just trained a real neural network on a dataset that would stump simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Step 7: Visualizing the Neural Network's Performance\n",
    "\n",
    "Now that we've trained our neural network using TensorFlow/Keras, let‚Äôs see what it actually learned.\n",
    "\n",
    "We'll look at:\n",
    "- The decision boundary: where the model predicts one class vs another\n",
    "- The loss curves: how the model improved during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_neural_network_results(X, y, model, scaler, history, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí¨ What Can We Conclude?\n",
    "\n",
    "- The **decision boundary** shows that the neural network learned to separate the circular classes quite well ‚Äî something **linear models could never do**.\n",
    "- The **loss curves** confirm that the model was able to reduce error during training and generalize well to unseen data (validation loss stays low).\n",
    "\n",
    "Neural networks **automatically learn complex patterns** through multiple layers and nonlinear activations ‚Äî that‚Äôs their power!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéÆ Explore Neural Networks Visually (Optional)\n",
    "\n",
    "Want to **play with neurons, layers, and activations** without writing code?\n",
    "\n",
    "üëâ Head over to the [**TensorFlow Playground**](https://playground.tensorflow.org/)!\n",
    "\n",
    "You can:\n",
    "- Add or remove hidden layers\n",
    "- Try different activation functions\n",
    "- Tune learning rates and regularization\n",
    "- Watch how the network adjusts to complex patterns like **concentric circles**\n",
    "\n",
    "üîç **Tip:** In the Playground, use the dataset with two circles (bottom-right option).  \n",
    "It's the same one we just trained on!\n",
    "\n",
    "This interactive tool is a perfect way to **see** what neural networks are learning ‚Äî in real time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üñºÔ∏è Deep Learning in Action: Recognizing Handwritten Digits\n",
    "\n",
    "We‚Äôve seen how neural networks can learn curved shapes ‚Äî but can they recognize **images**?\n",
    "\n",
    "In this example, we‚Äôll use real image data:\n",
    "- Each image is just **8√ó8 grayscale pixels**\n",
    "- Each one shows a handwritten **digit (0‚Äì9)**\n",
    "- Your goal: build a deep neural network to **recognize digits automatically**\n",
    "\n",
    "This is a **mini-version of image recognition** and shows how deep learning powers tasks like OCR (optical character recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X_digits, y_digits = digits.data, digits.target\n",
    "\n",
    "print(\"üñºÔ∏è Handwritten Digit Recognition Dataset:\")\n",
    "print(f\"   Images: {len(X_digits)}\")\n",
    "print(f\"   Pixels per image: {X_digits.shape[1]} (8x8 grid)\")\n",
    "print(f\"   Classes: {len(set(y_digits))} digits (0‚Äì9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÅÔ∏è‚Äçüó®Ô∏è Let‚Äôs Look at the Data\n",
    "\n",
    "Each digit is represented by a flat list of 64 numbers (8√ó8 grayscale pixels). Let‚Äôs visualize some of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digit_samples(X_digits, y_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Step 1: Prepare the Data\n",
    "\n",
    "We scale the pixel values (just like with the circles) and split the data into training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and scale the data\n",
    "X_train_digits, X_test_digits, y_train_digits, y_test_digits = train_test_split(\n",
    "    X_digits, y_digits, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler_digits = StandardScaler()\n",
    "X_train_digits_scaled = scaler_digits.fit_transform(X_train_digits)\n",
    "X_test_digits_scaled = scaler_digits.transform(X_test_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ Step 2: Build a Deep Neural Network\n",
    "\n",
    "We now build a **deeper network** that can:\n",
    "- Learn from all 64 input pixels\n",
    "- Use **ReLU activations**\n",
    "- Have **3 hidden layers** with decreasing size (100 ‚Üí 50 ‚Üí 25)\n",
    "- Predict the correct digit (0‚Äì9) using a **softmax output layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_digits = keras.Sequential([\n",
    "    layers.Input(shape=(64,)),  # 8x8 images flattened to 64 pixels\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(25, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # 10 output classes\n",
    "])\n",
    "\n",
    "model_digits.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_digits = model_digits.fit(\n",
    "    X_train_digits_scaled, y_train_digits,\n",
    "    epochs=30, validation_data=(X_test_digits_scaled, y_test_digits),\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Step 3: Check Accuracy\n",
    "\n",
    "Let‚Äôs see how well the model performs on both training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = model_digits.evaluate(X_train_digits_scaled, y_train_digits, verbose=0)\n",
    "test_loss, test_acc = model_digits.evaluate(X_test_digits_scaled, y_test_digits, verbose=0)\n",
    "\n",
    "print(f\"üéØ Deep Learning Results:\")\n",
    "print(f\"   Training Accuracy: {train_acc:.1%}\")\n",
    "print(f\"   Test Accuracy: {test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Step 4: Try the Network on New Digits\n",
    "\n",
    "Let‚Äôs ask the model to classify some unseen digits ‚Äî and show how confident it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 5 random digits from the test set\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(X_test_digits), size=5, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    true_label = y_test_digits[idx]\n",
    "    image = X_test_digits[idx].reshape(1, -1)\n",
    "    prediction = model_digits.predict(image, verbose=0)\n",
    "    predicted_label = np.argmax(prediction)\n",
    "    confidence = np.max(prediction)\n",
    "\n",
    "    print(f\"üî¢ True: {true_label} | Predicted: {predicted_label} | Confidence: {confidence:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Summary: What You Learned\n",
    "\n",
    "- Neural networks can go **beyond synthetic patterns** and handle real-world images.\n",
    "- With enough depth and training, they can classify digits with **over 95% accuracy**!\n",
    "- This is a simplified version of what powers real tools like OCR, CAPTCHA solvers, and postal scanners.\n",
    "\n",
    "Next steps:\n",
    "- Learn how **convolutional layers** can improve image recognition\n",
    "- Explore more complex datasets like **MNIST** or **CIFAR-10**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Try It Yourself: Digit Classifier in Action\n",
    "\n",
    "Use the slider below to explore how the neural network performs on different handwritten digits.\n",
    "\n",
    "Watch:\n",
    "- What it gets **right** ‚úÖ\n",
    "- What it **misclassifies** ‚ùå\n",
    "- How **confident** it is in its answers üîç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_interactive_digit_classifier(X_test_digits, y_test_digits, X_test_digits_scaled, model_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Bonus: Neural Networks in PyTorch\n",
    "\n",
    "Keras (TensorFlow) is great for fast prototyping ‚Äî but PyTorch offers flexibility and is widely used in research.\n",
    "\n",
    "Here‚Äôs how you can build and train the same neural network using **PyTorch**.\n",
    "\n",
    "We'll:\n",
    "- Define a custom neural network class\n",
    "- Use binary cross-entropy loss and the Adam optimizer\n",
    "- Train the model and evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Instantiate the model, define loss and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = model(X_test_tensor).numpy()\n",
    "    test_preds_class = (test_preds > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, test_preds_class)\n",
    "\n",
    "print(f\"‚úÖ PyTorch Test Accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ What‚Äôs Different?\n",
    "\n",
    "- You define your own `nn.Module` class to specify the architecture\n",
    "- Training uses a manual loop (gives you more control)\n",
    "- PyTorch works well with **autograd** and **GPU acceleration**\n",
    "\n",
    "Both Keras and PyTorch are excellent ‚Äî the choice often depends on the task and your preference.\n",
    "\n",
    "üß™ Try recreating this network for other datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Congratulations ‚Äì You've Completed the Machine Learning Series!\n",
    "\n",
    "Over the past three sessions, you've built a strong foundation in machine learning ‚Äì from simple models to deep neural networks powering modern AI.\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ Your Learning Journey\n",
    "\n",
    "**üîπ Class 1: Linear Models**\n",
    "- Discovered how machines learn from data with straight-line models  \n",
    "- Understood gradient descent as a tool for optimization  \n",
    "- Built the mathematical intuition behind model training  \n",
    "\n",
    "**üîπ Class 2: Decision Trees & Ensembles**  \n",
    "- Learned how models split data into interpretable rules  \n",
    "- Saw how combining many trees creates powerful predictors (Random Forests)  \n",
    "- Gained tools that balance accuracy and explainability  \n",
    "\n",
    "**üîπ Class 3: Neural Networks & Deep Learning**  \n",
    "- Explored how layered ‚Äúneurons‚Äù can learn complex, non-linear patterns  \n",
    "- Trained deep models to recognize patterns in visuals and data  \n",
    "- Saw how AI powers modern tools like voice assistants, image recognition, and language translation  \n",
    "\n",
    "---\n",
    "\n",
    "### üí° Big Takeaways\n",
    "\n",
    "- üß† **No single model fits all problems** ‚Äì know your tools  \n",
    "- üîç **Complex patterns require complex models**, but they also need more data and care  \n",
    "- ‚öñÔ∏è **Simplicity vs. performance is always a trade-off**  \n",
    "- üîÑ **Interpreting and validating results is as important as building the model**\n",
    "\n",
    "---\n",
    "\n",
    "### üåê Where Machine Learning Shows Up Around You\n",
    "\n",
    "- üé¨ **Netflix recommendations** ‚Äì collaborative filtering + neural nets  \n",
    "- üì∏ **Face recognition** ‚Äì deep convolutional networks  \n",
    "- ‚úâÔ∏è **Spam detection** ‚Äì decision trees on text features  \n",
    "- üöó **Self-driving perception** ‚Äì computer vision + reinforcement learning  \n",
    "- üåç **Real-time translation** ‚Äì sequence models and transformers  \n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ What's Next?\n",
    "\n",
    "**üîß Practice & Projects**\n",
    "- Apply what you‚Äôve learned to datasets from your field  \n",
    "- Build your first real-world ML mini-project  \n",
    "- Use platforms like Kaggle or Hugging Face to explore datasets and models  \n",
    "\n",
    "**üìö Keep Learning**\n",
    "- Dive into:\n",
    "  - Computer Vision\n",
    "  - Natural Language Processing\n",
    "  - Reinforcement Learning\n",
    "  - Generative AI  \n",
    "- Try TensorFlow, PyTorch, Scikit-Learn in more depth\n",
    "\n",
    "üíº **Start using machine learning in your field!**\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Final Wisdom\n",
    "\n",
    "You don‚Äôt need a PhD to start using machine learning effectively.\n",
    "\n",
    "What matters most:\n",
    "- üîç **Knowing which model fits the problem**\n",
    "- üß™ **Testing, validating, and improving your model**\n",
    "- üß† **Thinking critically about data and bias**\n",
    "- üì¢ **Explaining your results to others**\n",
    "\n",
    "---\n",
    "\n",
    "### üåü You‚Äôre Ready\n",
    "\n",
    "This is just the beginning ‚Äî ML is a journey of curiosity, experimentation, and creativity.\n",
    "\n",
    "Go explore, build, ask questions, and make something amazing.  \n",
    "We‚Äôre excited to see what you‚Äôll do. üöÄü§ñ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDV-ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
