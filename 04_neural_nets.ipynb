{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning Class 3: Neural Networks & Deep Learning\n",
    "\n",
    "Welcome to the third and final class in our machine learning journey!\n",
    "So far, weâ€™ve learned how:\n",
    "\n",
    "- Linear models uncover simple trends in data ğŸ“ˆ\n",
    "- Decision trees ask smart questions to classify and predict ğŸŒ³\n",
    "\n",
    "Today, we turn to the most powerful tool in modern AI:\n",
    "\n",
    "### âœ¨ Neural Networks â€” systems inspired by the brain that can learn almost anything.\n",
    "\n",
    "### ğŸ¯ What You'll Learn Today\n",
    "\n",
    "1. **Neural Network Basics** â€” How artificial neurons learn from data\n",
    "2. **Building Your First Network** â€” Step-by-step hands-on demo\n",
    "3. **Going Deeper** â€” Why â€œdeepâ€ learning is so powerful\n",
    "4. **Real Applications** â€” From image recognition to language translation\n",
    "5. **Bonus** â€” Learn how to build the same network in Keras and PyTorch\n",
    "\n",
    "### ğŸ§  **The Core Idea**\n",
    "\n",
    "Neural networks are made of layers of tiny computing units â€” neurons â€” that:\n",
    "\n",
    "- Take inputs (like pixel values or text features)\n",
    "- Apply weights and nonlinear activations\n",
    "- Pass results forward to make predictions\n",
    "\n",
    "With enough neurons and layers, these networks can:\n",
    "\n",
    "- Recognize complex images ğŸ–¼ï¸\n",
    "- Understand language and context ğŸŒ\n",
    "- Learn intricate patterns that no human could code by hand\n",
    "\n",
    "### ğŸ”— Building on Previous Classes\n",
    "\n",
    "| Class | Focus | Key Idea |\n",
    "|-------|-------|----------|\n",
    "| 1ï¸âƒ£ Linear Models | Fit lines to patterns | Models assume a simple structure |\n",
    "| 2ï¸âƒ£ Trees | Ask yes/no questions | Let data dictate structure |\n",
    "| 3ï¸âƒ£ Neural Nets | Learn anything | Model learns structure from scratch |\n",
    "\n",
    "Neural networks **donâ€™t need hand-coded rules**. They build their own understanding by adjusting internal parameters based on data â€” making them the foundation of everything from ChatGPT to self-driving cars.\n",
    "\n",
    "### ğŸŒ Why Neural Networks Matter\n",
    "\n",
    "They power many of the tools you use daily:\n",
    "\n",
    "- ğŸ“¸ Image classifiers on your phone\n",
    "- ğŸ§  Large Language Models like ChatGPT\n",
    "- ğŸ§ Music & movie recommendations\n",
    "- ğŸš— Autonomous vehicles\n",
    "- ğŸ§¬ Healthcare diagnostics\n",
    "\n",
    "And theyâ€™re just getting started.\n",
    "\n",
    "---\n",
    "\n",
    "Letâ€™s dive in and build one from scratch â€” then see how to train it using real-world libraries like TensorFlow/Keras and PyTorch.\n",
    "\n",
    "Get ready to enter the world of deep learning! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Setup - Import Our Neural Network Tools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.datasets import make_circles, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Input\n",
    "from ipywidgets import interact, FloatSlider, IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducible results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Neural Networks - Artificial Brains\n",
    "\n",
    "### ğŸ§  **How Does Your Brain Work?**\n",
    "\n",
    "Your brain has ~86 billion neurons that:\n",
    "1. **Receive signals** from other neurons\n",
    "2. **Process information** by combining signals\n",
    "3. **Send output** to other neurons if activated\n",
    "4. **Learn** by strengthening important connections\n",
    "\n",
    "**Artificial neural networks** are a *simplified version* of this idea:\n",
    "\n",
    "- They use numbers instead of electric signals.\n",
    "- They \"learn\" by changing weights through data and feedback.\n",
    "\n",
    "### ğŸ”— **From Biological to Artificial**\n",
    "\n",
    "Let's break down what a single **artificial neuron** does:\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Inputs** | Numbers representing features (like age, income, etc.) |\n",
    "| **Weights** | How important each input is (learned during training) |\n",
    "| **Activation** | Decides whether to \"fire\" based on weighted inputs |\n",
    "| **Output** | A number passed to the next layer |\n",
    "\n",
    "A **neural network** simply connects many of these neurons in layers:\n",
    "\n",
    "- **Input layer** takes your data\n",
    "- **Hidden layers** do the processing\n",
    "- **Output layer** gives the final prediction\n",
    "\n",
    "### ğŸ¯ Why Networks Beat Single Neurons\n",
    "\n",
    "- A single neuron can only draw a straight line â€” itâ€™s like linear regression.\n",
    "- A network with more neurons and layers can:\n",
    "  - Bend, curve, and twist boundaries ğŸŒ€\n",
    "  - Combine patterns from multiple inputs\n",
    "  - Capture non-linear structures in data\n",
    "\n",
    "Letâ€™s build some intuition with a real example. ğŸ“Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Example: Can a Single Neuron Predict Who Will Buy?\n",
    "\n",
    "Weâ€™ll use a tiny **customer dataset** with just two inputs: `age` and `income`.\n",
    "\n",
    "Each customer either bought a product (1) or did not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data = pd.DataFrame({\n",
    "    'age': [25, 35, 45, 55, 30, 40, 50, 28, 38, 48],\n",
    "    'income': [30, 50, 70, 90, 40, 60, 80, 35, 55, 75],\n",
    "    'will_buy': [0, 0, 1, 1, 0, 1, 1, 0, 0, 1]\n",
    "})\n",
    "print(customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ˆ Let's Visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(customer_data, x='age', y='income', color='will_buy',\n",
    "                color_discrete_map={0: 'red', 1: 'green'},\n",
    "                title=\"ğŸ›’ Customer Purchase Patterns\",\n",
    "                labels={'will_buy': 'Will Buy'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª Testing a Single Neuron\n",
    "\n",
    "Letâ€™s simulate how one artificial neuron would handle a single customer. Let's assume the custom is 40 years old with an income of $60,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_neuron(age, income, weight_age=0.1, weight_income=0.05, bias=-3):\n",
    "    weighted_sum = weight_age * age + weight_income * income + bias\n",
    "\n",
    "    # Let's use a sigmoid activation function\n",
    "    activation = 1 / (1 + np.exp(-weighted_sum))\n",
    "    prediction = 1 if activation > 0.5 else 0\n",
    "\n",
    "    return prediction, activation\n",
    "\n",
    "prediction, confidence = simple_neuron(age=40, income=60)\n",
    "print(f\"Prediction: {'Will Buy' if prediction == 1 else 'Will Not Buy'}, Confidence: {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ® Interactive: Design Your Own Neuron\n",
    "\n",
    "The following code defines a helper function to visualize how a single artificial neuron makes predictions.  \n",
    "Donâ€™t worry if it looks long â€” it's just setting up the plotting and interaction.\n",
    "\n",
    "Now, use the sliders below to adjust the weights and bias and see how your neuron performs!  \n",
    "Can you find a setting that gets most predictions right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(\n",
    "    weight_age=FloatSlider(value=0.1, min=-0.5, max=0.5, step=0.01, description='Weight (Age)'),\n",
    "    weight_income=FloatSlider(value=0.05, min=-0.5, max=0.5, step=0.01, description='Weight (Income)'),\n",
    "    bias=FloatSlider(value=-3.0, min=-10, max=10, step=0.1, description='Bias')\n",
    ")\n",
    "def design_neuron(weight_age, weight_income, bias):\n",
    "    \"\"\"Interactive tool to visualize a single neuron's decision boundary\"\"\"\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "\n",
    "    for _, row in customer_data.iterrows():\n",
    "        pred, conf = simple_neuron(row['age'], row['income'], weight_age, weight_income, bias)\n",
    "        predictions.append(pred)\n",
    "        confidences.append(conf)\n",
    "\n",
    "    accuracy = sum(p == t for p, t in zip(predictions, customer_data['will_buy'])) / len(predictions)\n",
    "\n",
    "    # Set up plot\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\"Neuron Predictions\", \"Decision Boundary\"),\n",
    "        specs=[[{\"type\": \"scatter\"}, {\"type\": \"contour\"}]]\n",
    "    )\n",
    "\n",
    "    # Left plot: correct (green) vs wrong (red) predictions\n",
    "    colors = ['green' if p == t else 'red' for p, t in zip(predictions, customer_data['will_buy'])]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=customer_data['age'],\n",
    "            y=customer_data['income'],\n",
    "            mode='markers',\n",
    "            marker=dict(size=12, color=colors),\n",
    "            name='Predictions'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Right plot: decision boundary via contour\n",
    "    age_range = np.linspace(20, 60, 50)\n",
    "    income_range = np.linspace(20, 100, 50)\n",
    "    Age, Income = np.meshgrid(age_range, income_range)\n",
    "\n",
    "    Z = np.zeros_like(Age)\n",
    "    for i in range(Age.shape[0]):\n",
    "        for j in range(Age.shape[1]):\n",
    "            _, conf = simple_neuron(Age[i, j], Income[i, j], weight_age, weight_income, bias)\n",
    "            Z[i, j] = conf\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Contour(\n",
    "            x=age_range,\n",
    "            y=income_range,\n",
    "            z=Z,\n",
    "            colorscale='RdYlGn',\n",
    "            contours=dict(start=0, end=1, size=0.1),\n",
    "            showscale=False,\n",
    "            name='Decision Boundary'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # Actual data points on decision plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=customer_data['age'],\n",
    "            y=customer_data['income'],\n",
    "            mode='markers',\n",
    "            marker=dict(size=10, color=customer_data['will_buy'],\n",
    "                        colorscale='RdYlGn', line=dict(width=2, color='black')),\n",
    "            name='Actual Data'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=400,\n",
    "        title_text=f\"ğŸ§  Your Neuron: {accuracy:.1%} Accuracy\"\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Age\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Income ($k)\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Age\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Income ($k)\", row=1, col=2)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Console summary\n",
    "    print(f\"ğŸ§  Your Neuron Performance:\")\n",
    "    print(f\"   Accuracy: {accuracy:.1%}\")\n",
    "    print(f\"   Weights: Age={weight_age:.2f}, Income={weight_income:.3f}\")\n",
    "    print(f\"   Bias: {bias:.2f}\")\n",
    "\n",
    "    if accuracy >= 0.8:\n",
    "        print(\"\\nğŸ‰ Excellent! Your neuron learned the pattern well!\")\n",
    "    elif accuracy >= 0.6:\n",
    "        print(\"\\nâœ… Good! Try adjusting weights for better performance.\")\n",
    "    else:\n",
    "        print(\"\\nğŸ¤” Keep experimenting with the weights and bias!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What This Does:\n",
    "\n",
    "- It shows you how well your weights perform.\n",
    "- The first plot shows which predictions were right (green) or wrong (red).\n",
    "- The second plot shows your neuronâ€™s decision boundary: where it predicts â€œyesâ€ vs â€œnoâ€.\n",
    "\n",
    "### Interpreting the Output:\n",
    "\n",
    "- A boundary that cuts through the data well means your neuron learned something useful!\n",
    "- A boundary that misses many points means the neuron is too simple â€” or poorly tuned.\n",
    "- Look at the accuracy printed â€” aim for at least 80%!\n",
    "\n",
    "ğŸ§  This helps you build intuition for:\n",
    "- What weights and bias do\n",
    "- How decision boundaries work\n",
    "- Why real neural networks need more neurons and more layers\n",
    "\n",
    "### ğŸ’¡ Key Takeaways\n",
    "\n",
    "- A single neuron is powerful but limited.\n",
    "- It struggles with curved or complex boundaries.\n",
    "- You just discovered why we need multi-layer neural networks â€” to handle complex patterns in data!\n",
    "\n",
    "ğŸ‘‰ Next, weâ€™ll move from one neuron to an actual network, and see how it improves performance dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building Your First Neural Network\n",
    "\n",
    "### ğŸ”„ From a Single Neuron to a Learning Machine\n",
    "\n",
    "A single neuron can draw a line â€” but it canâ€™t capture curves, corners, or complex decision boundaries.  \n",
    "To move beyond that, we need **many neurons** working together in **layers**.\n",
    "\n",
    "This is the core idea of a **neural network**.\n",
    "\n",
    "### ğŸ§± What Happens Inside a Neural Network?\n",
    "\n",
    "- The **input layer** passes features to the network\n",
    "- **Hidden layers** process information step by step:\n",
    "  - The first layer might detect simple signals\n",
    "  - The next layer combines them into patterns\n",
    "  - Later layers build more abstract understanding\n",
    "- The **output layer** makes the final prediction\n",
    "\n",
    "### ğŸŒŸ Why Is This Powerful?\n",
    "\n",
    "- **Multiple neurons** allow flexible, curved decision boundaries  \n",
    "- **Multiple layers** allow abstraction: from pixels â†’ edges â†’ shapes â†’ faces  \n",
    "- **Activation functions** give the network its non-linear magic\n",
    "\n",
    "With the right architecture, a neural network can learn just about anything.\n",
    "\n",
    "Letâ€™s build one!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸŒ€ Step 1: Create a Challenging Dataset\n",
    "\n",
    "Weâ€™ll now create a dataset that **cannot be solved with straight lines**.  \n",
    "It has circular patterns â€” something only neural networks can untangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex dataset that needs a neural network\n",
    "X, y = make_circles(n_samples=300, noise=0.1, factor=0.3, random_state=42)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "circle_data = pd.DataFrame(X, columns=['x', 'y'])\n",
    "circle_data['class'] = y\n",
    "\n",
    "print(\"ğŸ¯ Complex Pattern Dataset:\")\n",
    "print(f\"   Data points: {len(circle_data)}\")\n",
    "print(f\"   Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Step 2: Visualize the Problem\n",
    "\n",
    "Letâ€™s plot the data to see why this is tricky.  \n",
    "The two classes form **concentric circles** â€” a single line wonâ€™t separate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the complex pattern\n",
    "fig = px.scatter(circle_data, x='x', y='y', color='class',\n",
    "                color_discrete_map={0: 'red', 1: 'blue'},\n",
    "                title=\"ğŸ¯ Complex Pattern: Circles within Circles\")\n",
    "\n",
    "fig.update_layout(height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§  **Observation**:  \n",
    "- âŒ **Linear models** will fail here  \n",
    "- âŒ **Single neurons** wonâ€™t help  \n",
    "- âœ… **Neural networks** are up for the challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ğŸ› ï¸ Step 3: Prepare the Data\n",
    "\n",
    "Weâ€™ll now split the data and scale it.  \n",
    "**Why scale?** Neural networks are sensitive to the scale of input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the data (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤– Step 4: Build and Train a Neural Network\n",
    "\n",
    "Letâ€™s create a real neural network with:\n",
    "- **2 hidden layers**\n",
    "- **10 neurons each**\n",
    "- **ReLU activation**\n",
    "- Up to **1000 training iterations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network with Keras\n",
    "model = keras.Sequential([\n",
    "    Input(shape=(2,)),\n",
    "    layers.Dense(10, activation='relu'),\n",
    "    layers.Dense(10, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model and store training history\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, verbose=0, validation_data=(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ˆ Step 5: Evaluate Performance\n",
    "\n",
    "Now letâ€™s test the networkâ€™s accuracy and see how well it learned the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training and test sets\n",
    "train_loss, train_accuracy = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "# Report results\n",
    "print(f\"\\nğŸ§  Neural Network Results:\")\n",
    "print(f\"   Training Accuracy: {train_accuracy:.1%}\")\n",
    "print(f\"   Test Accuracy: {test_accuracy:.1%}\")\n",
    "print(f\"   Network Architecture: Input â†’ 10 â†’ 10 â†’ Output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‰ Great job! You've just trained a real neural network on a dataset that would stump simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Step 7: Visualizing the Neural Network's Performance\n",
    "\n",
    "Now that we've trained our neural network using TensorFlow/Keras, letâ€™s see what it actually learned.\n",
    "\n",
    "We'll look at:\n",
    "- The decision boundary: where the model predicts one class vs another\n",
    "- The loss curves: how the model improved during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create meshgrid for decision boundary\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Predict over the grid\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_points_scaled = scaler.transform(grid_points)\n",
    "Z = model.predict(grid_points_scaled, verbose=0)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Neural Network Decision Boundary', 'Training Loss Curve'),\n",
    "    specs=[[{\"type\": \"contour\"}, {\"type\": \"xy\"}]]\n",
    ")\n",
    "\n",
    "# Subplot 1: Decision boundary\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=np.arange(x_min, x_max, h),\n",
    "        y=np.arange(y_min, y_max, h),\n",
    "        z=Z,\n",
    "        colorscale='RdYlBu',\n",
    "        showscale=False,\n",
    "        contours=dict(start=0, end=1, size=0.05),\n",
    "        opacity=0.6,\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "point_colors = ['red' if label == 0 else 'blue' for label in y]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X[:, 0], y=X[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color=point_colors,\n",
    "            line=dict(width=1, color='black'),\n",
    "            size=8\n",
    "        ),\n",
    "        name='Data Points'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Subplot 2: Training and validation loss\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(1, len(history.history['loss']) + 1)),\n",
    "        y=history.history['loss'],\n",
    "        mode='lines',\n",
    "        name='Training Loss',\n",
    "        line=dict(width=3, color='green')\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(1, len(history.history['val_loss']) + 1)),\n",
    "        y=history.history['val_loss'],\n",
    "        mode='lines',\n",
    "        name='Validation Loss',\n",
    "        line=dict(width=3, dash='dash', color='blue')\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Layout\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    title_text=f\"ğŸ§  Neural Network: Decision Boundary and Loss Curve (Test Accuracy: {test_accuracy:.1%})\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"X1\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"X2\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Epoch\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Loss\", row=1, col=2)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¬ What Can We Conclude?\n",
    "\n",
    "- The **decision boundary** shows that the neural network learned to separate the circular classes quite well â€” something **linear models could never do**.\n",
    "- The **loss curves** confirm that the model was able to reduce error during training and generalize well to unseen data (validation loss stays low).\n",
    "\n",
    "Neural networks **automatically learn complex patterns** through multiple layers and nonlinear activations â€” thatâ€™s their power!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ® Explore Neural Networks Visually (Optional)\n",
    "\n",
    "Want to **play with neurons, layers, and activations** without writing code?\n",
    "\n",
    "ğŸ‘‰ Head over to the [**TensorFlow Playground**](https://playground.tensorflow.org/)!\n",
    "\n",
    "You can:\n",
    "- Add or remove hidden layers\n",
    "- Try different activation functions\n",
    "- Tune learning rates and regularization\n",
    "- Watch how the network adjusts to complex patterns like **concentric circles**\n",
    "\n",
    "ğŸ” **Tip:** In the Playground, use the dataset with two circles (bottom-right option).  \n",
    "It's the same one we just trained on!\n",
    "\n",
    "This interactive tool is a perfect way to **see** what neural networks are learning â€” in real time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ–¼ï¸ Deep Learning in Action: Recognizing Handwritten Digits\n",
    "\n",
    "Weâ€™ve seen how neural networks can learn curved shapes â€” but can they recognize **images**?\n",
    "\n",
    "In this example, weâ€™ll use real image data:\n",
    "- Each image is just **8Ã—8 grayscale pixels**\n",
    "- Each one shows a handwritten **digit (0â€“9)**\n",
    "- Your goal: build a deep neural network to **recognize digits automatically**\n",
    "\n",
    "This is a **mini-version of image recognition** and shows how deep learning powers tasks like OCR (optical character recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X_digits, y_digits = digits.data, digits.target\n",
    "\n",
    "print(\"ğŸ–¼ï¸ Handwritten Digit Recognition Dataset:\")\n",
    "print(f\"   Images: {len(X_digits)}\")\n",
    "print(f\"   Pixels per image: {X_digits.shape[1]} (8x8 grid)\")\n",
    "print(f\"   Classes: {len(set(y_digits))} digits (0â€“9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‘ï¸â€ğŸ—¨ï¸ Letâ€™s Look at the Data\n",
    "\n",
    "Each digit is represented by a flat list of 64 numbers (8Ã—8 grayscale pixels). Letâ€™s visualize some of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example digits\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=5,\n",
    "    subplot_titles=[f'Digit {i}' for i in range(10)]\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    idx = (y_digits == i).nonzero()[0][0]\n",
    "    image = X_digits[idx].reshape(8, 8)\n",
    "\n",
    "    row = 1 if i < 5 else 2\n",
    "    col = (i % 5) + 1\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=image, colorscale='gray', showscale=False),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"ğŸ–¼ï¸ Sample Handwritten Digits (8x8 pixels)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§  Step 1: Prepare the Data\n",
    "\n",
    "We scale the pixel values (just like with the circles) and split the data into training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and scale the data\n",
    "X_train_digits, X_test_digits, y_train_digits, y_test_digits = train_test_split(\n",
    "    X_digits, y_digits, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler_digits = StandardScaler()\n",
    "X_train_digits_scaled = scaler_digits.fit_transform(X_train_digits)\n",
    "X_test_digits_scaled = scaler_digits.transform(X_test_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤– Step 2: Build a Deep Neural Network\n",
    "\n",
    "We now build a **deeper network** that can:\n",
    "- Learn from all 64 input pixels\n",
    "- Use **ReLU activations**\n",
    "- Have **3 hidden layers** with decreasing size (100 â†’ 50 â†’ 25)\n",
    "- Predict the correct digit (0â€“9) using a **softmax output layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_digits = keras.Sequential([\n",
    "    layers.Input(shape=(64,)),  # 8x8 images flattened to 64 pixels\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(25, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # 10 output classes\n",
    "])\n",
    "\n",
    "model_digits.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_digits = model_digits.fit(\n",
    "    X_train_digits_scaled, y_train_digits,\n",
    "    epochs=30, validation_data=(X_test_digits_scaled, y_test_digits),\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ˆ Step 3: Check Accuracy\n",
    "\n",
    "Letâ€™s see how well the model performs on both training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = model_digits.evaluate(X_train_digits_scaled, y_train_digits, verbose=0)\n",
    "test_loss, test_acc = model_digits.evaluate(X_test_digits_scaled, y_test_digits, verbose=0)\n",
    "\n",
    "print(f\"ğŸ¯ Deep Learning Results:\")\n",
    "print(f\"   Training Accuracy: {train_acc:.1%}\")\n",
    "print(f\"   Test Accuracy: {test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Step 4: Try the Network on New Digits\n",
    "\n",
    "Letâ€™s ask the model to classify some unseen digits â€” and show how confident it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 5 random digits from the test set\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(X_test_digits), size=5, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    true_label = y_test_digits[idx]\n",
    "    image = X_test_digits[idx].reshape(1, -1)\n",
    "    prediction = model_digits.predict(image, verbose=0)\n",
    "    predicted_label = np.argmax(prediction)\n",
    "    confidence = np.max(prediction)\n",
    "\n",
    "    print(f\"ğŸ”¢ True: {true_label} | Predicted: {predicted_label} | Confidence: {confidence:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Summary: What You Learned\n",
    "\n",
    "- Neural networks can go **beyond synthetic patterns** and handle real-world images.\n",
    "- With enough depth and training, they can classify digits with **over 95% accuracy**!\n",
    "- This is a simplified version of what powers real tools like OCR, CAPTCHA solvers, and postal scanners.\n",
    "\n",
    "Next steps:\n",
    "- Learn how **convolutional layers** can improve image recognition\n",
    "- Explore more complex datasets like **MNIST** or **CIFAR-10**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ® Try It Yourself: Digit Classifier in Action\n",
    "\n",
    "Use the slider below to explore how the neural network performs on different handwritten digits.\n",
    "\n",
    "Watch:\n",
    "- What it gets **right** âœ…\n",
    "- What it **misclassifies** âŒ\n",
    "- How **confident** it is in its answers ğŸ”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(\n",
    "    digit_index=IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=len(X_test_digits) - 1,\n",
    "        step=1,\n",
    "        description='Digit #:',\n",
    "        continuous_update=False\n",
    "    )\n",
    ")\n",
    "def test_digit_classifier_keras(digit_index):\n",
    "    \"\"\"Interactive digit classification tool using Keras model\"\"\"\n",
    "\n",
    "    # Safeguard against out-of-bounds index\n",
    "    if digit_index >= len(X_test_digits):\n",
    "        digit_index = len(X_test_digits) - 1\n",
    "\n",
    "    test_image = X_test_digits[digit_index]\n",
    "    true_label = y_test_digits[digit_index]\n",
    "\n",
    "    # Predict with Keras model\n",
    "    input_scaled = X_test_digits_scaled[digit_index].reshape(1, -1)\n",
    "    probabilities = model_digits.predict(input_scaled, verbose=0)[0]\n",
    "    prediction = np.argmax(probabilities)\n",
    "\n",
    "    # Create side-by-side plots\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('ğŸ–¼ï¸ Test Image', 'ğŸ“Š Network Prediction'),\n",
    "        specs=[[{\"type\": \"heatmap\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "\n",
    "    # Left: Show the digit image\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=test_image.reshape(8, 8), colorscale='gray', showscale=False),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Right: Bar chart of predicted probabilities\n",
    "    digits = list(range(10))\n",
    "    bar_colors = ['green' if i == prediction else 'lightblue' for i in digits]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=digits, y=probabilities, marker_color=bar_colors),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # Add vertical line for the true label\n",
    "    fig.add_vline(x=true_label, line_dash=\"dash\", line_color=\"red\",\n",
    "                  annotation_text=f\"True: {true_label}\", row=1, col=2)\n",
    "\n",
    "    # Prepare summary\n",
    "    result = \"âœ… CORRECT\" if prediction == true_label else \"âŒ WRONG\"\n",
    "    confidence = probabilities[prediction]\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=400,\n",
    "        title_text=f\"ğŸ§  Prediction: {prediction} (True: {true_label}) â€” {result} ({confidence:.1%} confidence)\"\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Pixel Position\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Digit\", row=1, col=2, tickmode='linear', dtick=1)\n",
    "    fig.update_yaxes(title_text=\"Pixel Position\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Probability\", row=1, col=2)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    print(f\"ğŸ¯ Classification Summary:\")\n",
    "    print(f\"   True Digit: {true_label}\")\n",
    "    print(f\"   Predicted : {prediction}\")\n",
    "    print(f\"   Confidence: {confidence:.1%}\")\n",
    "    print(f\"   Result    : {result}\")\n",
    "\n",
    "    top_3 = np.argsort(probabilities)[-3:][::-1]\n",
    "    print(f\"\\nğŸ† Top 3 Predictions:\")\n",
    "    for rank, idx in enumerate(top_3, start=1):\n",
    "        print(f\"   {rank}. Digit {idx}: {probabilities[idx]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Bonus: Neural Networks in PyTorch\n",
    "\n",
    "Keras (TensorFlow) is great for fast prototyping â€” but PyTorch offers flexibility and is widely used in research.\n",
    "\n",
    "Hereâ€™s how you can build and train the same neural network using **PyTorch**.\n",
    "\n",
    "We'll:\n",
    "- Define a custom neural network class\n",
    "- Use binary cross-entropy loss and the Adam optimizer\n",
    "- Train the model and evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Instantiate the model, define loss and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = model(X_test_tensor).numpy()\n",
    "    test_preds_class = (test_preds > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, test_preds_class)\n",
    "\n",
    "print(f\"âœ… PyTorch Test Accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”„ Whatâ€™s Different?\n",
    "\n",
    "- You define your own `nn.Module` class to specify the architecture\n",
    "- Training uses a manual loop (gives you more control)\n",
    "- PyTorch works well with **autograd** and **GPU acceleration**\n",
    "\n",
    "Both Keras and PyTorch are excellent â€” the choice often depends on the task and your preference.\n",
    "\n",
    "ğŸ§ª Try recreating this network for other datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Congratulations â€“ You've Completed the Machine Learning Series!\n",
    "\n",
    "Over the past three sessions, you've built a strong foundation in machine learning â€“ from simple models to deep neural networks powering modern AI.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§­ Your Learning Journey\n",
    "\n",
    "**ğŸ”¹ Class 1: Linear Models**\n",
    "- Discovered how machines learn from data with straight-line models  \n",
    "- Understood gradient descent as a tool for optimization  \n",
    "- Built the mathematical intuition behind model training  \n",
    "\n",
    "**ğŸ”¹ Class 2: Decision Trees & Ensembles**  \n",
    "- Learned how models split data into interpretable rules  \n",
    "- Saw how combining many trees creates powerful predictors (Random Forests)  \n",
    "- Gained tools that balance accuracy and explainability  \n",
    "\n",
    "**ğŸ”¹ Class 3: Neural Networks & Deep Learning**  \n",
    "- Explored how layered â€œneuronsâ€ can learn complex, non-linear patterns  \n",
    "- Trained deep models to recognize patterns in visuals and data  \n",
    "- Saw how AI powers modern tools like voice assistants, image recognition, and language translation  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Big Takeaways\n",
    "\n",
    "- ğŸ§  **No single model fits all problems** â€“ know your tools  \n",
    "- ğŸ” **Complex patterns require complex models**, but they also need more data and care  \n",
    "- âš–ï¸ **Simplicity vs. performance is always a trade-off**  \n",
    "- ğŸ”„ **Interpreting and validating results is as important as building the model**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒ Where Machine Learning Shows Up Around You\n",
    "\n",
    "- ğŸ¬ **Netflix recommendations** â€“ collaborative filtering + neural nets  \n",
    "- ğŸ“¸ **Face recognition** â€“ deep convolutional networks  \n",
    "- âœ‰ï¸ **Spam detection** â€“ decision trees on text features  \n",
    "- ğŸš— **Self-driving perception** â€“ computer vision + reinforcement learning  \n",
    "- ğŸŒ **Real-time translation** â€“ sequence models and transformers  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ What's Next?\n",
    "\n",
    "**ğŸ”§ Practice & Projects**\n",
    "- Apply what youâ€™ve learned to datasets from your field  \n",
    "- Build your first real-world ML mini-project  \n",
    "- Use platforms like Kaggle or Hugging Face to explore datasets and models  \n",
    "\n",
    "**ğŸ“š Keep Learning**\n",
    "- Dive into:\n",
    "  - Computer Vision\n",
    "  - Natural Language Processing\n",
    "  - Reinforcement Learning\n",
    "  - Generative AI  \n",
    "- Try TensorFlow, PyTorch, Scikit-Learn in more depth\n",
    "\n",
    "ğŸ’¼ **Start using machine learning in your field!**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ› ï¸ Final Wisdom\n",
    "\n",
    "You donâ€™t need a PhD to start using machine learning effectively.\n",
    "\n",
    "What matters most:\n",
    "- ğŸ” **Knowing which model fits the problem**\n",
    "- ğŸ§ª **Testing, validating, and improving your model**\n",
    "- ğŸ§  **Thinking critically about data and bias**\n",
    "- ğŸ“¢ **Explaining your results to others**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒŸ Youâ€™re Ready\n",
    "\n",
    "This is just the beginning â€” ML is a journey of curiosity, experimentation, and creativity.\n",
    "\n",
    "Go explore, build, ask questions, and make something amazing.  \n",
    "Weâ€™re excited to see what youâ€™ll do. ğŸš€ğŸ¤–\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDV-ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
