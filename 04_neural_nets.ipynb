{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning Class 3: Neural Networks & Deep Learning\n",
    "\n",
    "Welcome to the third and final class in our machine learning journey!\n",
    "So far, we’ve learned how:\n",
    "\n",
    "- Linear models uncover simple trends in data 📈\n",
    "- Decision trees ask smart questions to classify and predict 🌳\n",
    "\n",
    "Today, we turn to the most powerful tool in modern AI:\n",
    "\n",
    "### ✨ Neural Networks — systems inspired by the brain that can learn almost anything.\n",
    "\n",
    "### 🎯 What You'll Learn Today\n",
    "\n",
    "1. **Neural Network Basics** — How artificial neurons learn from data\n",
    "2. **Building Your First Network** — Step-by-step hands-on demo\n",
    "3. **Going Deeper** — Why “deep” learning is so powerful\n",
    "4. **Real Applications** — From image recognition to language translation\n",
    "5. **Bonus** — Learn how to build the same network in Keras and PyTorch\n",
    "\n",
    "### 🧠 **The Core Idea**\n",
    "\n",
    "Neural networks are made of layers of tiny computing units — neurons — that:\n",
    "\n",
    "- Take inputs (like pixel values or text features)\n",
    "- Apply weights and nonlinear activations\n",
    "- Pass results forward to make predictions\n",
    "\n",
    "With enough neurons and layers, these networks can:\n",
    "\n",
    "- Recognize complex images 🖼️\n",
    "- Understand language and context 🌍\n",
    "- Learn intricate patterns that no human could code by hand\n",
    "\n",
    "### 🔗 Building on Previous Classes\n",
    "\n",
    "| Class | Focus | Key Idea |\n",
    "|-------|-------|----------|\n",
    "| 1️⃣ Linear Models | Fit lines to patterns | Models assume a simple structure |\n",
    "| 2️⃣ Trees | Ask yes/no questions | Let data dictate structure |\n",
    "| 3️⃣ Neural Nets | Learn anything | Model learns structure from scratch |\n",
    "\n",
    "Neural networks **don’t need hand-coded rules**. They build their own understanding by adjusting internal parameters based on data — making them the foundation of everything from ChatGPT to self-driving cars.\n",
    "\n",
    "### 🌍 Why Neural Networks Matter\n",
    "\n",
    "They power many of the tools you use daily:\n",
    "\n",
    "- 📸 Image classifiers on your phone\n",
    "- 🧠 Large Language Models like ChatGPT\n",
    "- 🎧 Music & movie recommendations\n",
    "- 🚗 Autonomous vehicles\n",
    "- 🧬 Healthcare diagnostics\n",
    "\n",
    "And they’re just getting started.\n",
    "\n",
    "---\n",
    "\n",
    "Let’s dive in and build one from scratch — then see how to train it using real-world libraries like TensorFlow/Keras and PyTorch.\n",
    "\n",
    "Get ready to enter the world of deep learning! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Setup - Import Our Neural Network Tools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.datasets import make_circles, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Input\n",
    "from ipywidgets import interact, FloatSlider, IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducible results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Neural Networks - Artificial Brains\n",
    "\n",
    "### 🧠 **How Does Your Brain Work?**\n",
    "\n",
    "Your brain has ~86 billion neurons that:\n",
    "1. **Receive signals** from other neurons\n",
    "2. **Process information** by combining signals\n",
    "3. **Send output** to other neurons if activated\n",
    "4. **Learn** by strengthening important connections\n",
    "\n",
    "**Artificial neural networks** are a *simplified version* of this idea:\n",
    "\n",
    "- They use numbers instead of electric signals.\n",
    "- They \"learn\" by changing weights through data and feedback.\n",
    "\n",
    "### 🔗 **From Biological to Artificial**\n",
    "\n",
    "Let's break down what a single **artificial neuron** does:\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Inputs** | Numbers representing features (like age, income, etc.) |\n",
    "| **Weights** | How important each input is (learned during training) |\n",
    "| **Activation** | Decides whether to \"fire\" based on weighted inputs |\n",
    "| **Output** | A number passed to the next layer |\n",
    "\n",
    "A **neural network** simply connects many of these neurons in layers:\n",
    "\n",
    "- **Input layer** takes your data\n",
    "- **Hidden layers** do the processing\n",
    "- **Output layer** gives the final prediction\n",
    "\n",
    "### 🎯 Why Networks Beat Single Neurons\n",
    "\n",
    "- A single neuron can only draw a straight line — it’s like linear regression.\n",
    "- A network with more neurons and layers can:\n",
    "  - Bend, curve, and twist boundaries 🌀\n",
    "  - Combine patterns from multiple inputs\n",
    "  - Capture non-linear structures in data\n",
    "\n",
    "Let’s build some intuition with a real example. 📊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 Example: Can a Single Neuron Predict Who Will Buy?\n",
    "\n",
    "We’ll use a tiny **customer dataset** with just two inputs: `age` and `income`.\n",
    "\n",
    "Each customer either bought a product (1) or did not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data = pd.DataFrame({\n",
    "    'age': [25, 35, 45, 55, 30, 40, 50, 28, 38, 48],\n",
    "    'income': [30, 50, 70, 90, 40, 60, 80, 35, 55, 75],\n",
    "    'will_buy': [0, 0, 1, 1, 0, 1, 1, 0, 0, 1]\n",
    "})\n",
    "print(customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📈 Let's Visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(customer_data, x='age', y='income', color='will_buy',\n",
    "                color_discrete_map={0: 'red', 1: 'green'},\n",
    "                title=\"🛒 Customer Purchase Patterns\",\n",
    "                labels={'will_buy': 'Will Buy'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 Testing a Single Neuron\n",
    "\n",
    "Let’s simulate how one artificial neuron would handle a single customer. Let's assume the custom is 40 years old with an income of $60,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_neuron(age, income, weight_age=0.1, weight_income=0.05, bias=-3):\n",
    "    weighted_sum = weight_age * age + weight_income * income + bias\n",
    "\n",
    "    # Let's use a sigmoid activation function\n",
    "    activation = 1 / (1 + np.exp(-weighted_sum))\n",
    "    prediction = 1 if activation > 0.5 else 0\n",
    "\n",
    "    return prediction, activation\n",
    "\n",
    "prediction, confidence = simple_neuron(age=40, income=60)\n",
    "print(f\"Prediction: {'Will Buy' if prediction == 1 else 'Will Not Buy'}, Confidence: {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎮 Interactive: Design Your Own Neuron\n",
    "\n",
    "The following code defines a helper function to visualize how a single artificial neuron makes predictions.  \n",
    "Don’t worry if it looks long — it's just setting up the plotting and interaction.\n",
    "\n",
    "Now, use the sliders below to adjust the weights and bias and see how your neuron performs!  \n",
    "Can you find a setting that gets most predictions right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(\n",
    "    weight_age=FloatSlider(value=0.1, min=-0.5, max=0.5, step=0.01, description='Weight (Age)'),\n",
    "    weight_income=FloatSlider(value=0.05, min=-0.5, max=0.5, step=0.01, description='Weight (Income)'),\n",
    "    bias=FloatSlider(value=-3.0, min=-10, max=10, step=0.1, description='Bias')\n",
    ")\n",
    "def design_neuron(weight_age, weight_income, bias):\n",
    "    \"\"\"Interactive tool to visualize a single neuron's decision boundary\"\"\"\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "\n",
    "    for _, row in customer_data.iterrows():\n",
    "        pred, conf = simple_neuron(row['age'], row['income'], weight_age, weight_income, bias)\n",
    "        predictions.append(pred)\n",
    "        confidences.append(conf)\n",
    "\n",
    "    accuracy = sum(p == t for p, t in zip(predictions, customer_data['will_buy'])) / len(predictions)\n",
    "\n",
    "    # Set up plot\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\"Neuron Predictions\", \"Decision Boundary\"),\n",
    "        specs=[[{\"type\": \"scatter\"}, {\"type\": \"contour\"}]]\n",
    "    )\n",
    "\n",
    "    # Left plot: correct (green) vs wrong (red) predictions\n",
    "    colors = ['green' if p == t else 'red' for p, t in zip(predictions, customer_data['will_buy'])]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=customer_data['age'],\n",
    "            y=customer_data['income'],\n",
    "            mode='markers',\n",
    "            marker=dict(size=12, color=colors),\n",
    "            name='Predictions'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Right plot: decision boundary via contour\n",
    "    age_range = np.linspace(20, 60, 50)\n",
    "    income_range = np.linspace(20, 100, 50)\n",
    "    Age, Income = np.meshgrid(age_range, income_range)\n",
    "\n",
    "    Z = np.zeros_like(Age)\n",
    "    for i in range(Age.shape[0]):\n",
    "        for j in range(Age.shape[1]):\n",
    "            _, conf = simple_neuron(Age[i, j], Income[i, j], weight_age, weight_income, bias)\n",
    "            Z[i, j] = conf\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Contour(\n",
    "            x=age_range,\n",
    "            y=income_range,\n",
    "            z=Z,\n",
    "            colorscale='RdYlGn',\n",
    "            contours=dict(start=0, end=1, size=0.1),\n",
    "            showscale=False,\n",
    "            name='Decision Boundary'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # Actual data points on decision plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=customer_data['age'],\n",
    "            y=customer_data['income'],\n",
    "            mode='markers',\n",
    "            marker=dict(size=10, color=customer_data['will_buy'],\n",
    "                        colorscale='RdYlGn', line=dict(width=2, color='black')),\n",
    "            name='Actual Data'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=400,\n",
    "        title_text=f\"🧠 Your Neuron: {accuracy:.1%} Accuracy\"\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Age\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Income ($k)\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Age\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Income ($k)\", row=1, col=2)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Console summary\n",
    "    print(f\"🧠 Your Neuron Performance:\")\n",
    "    print(f\"   Accuracy: {accuracy:.1%}\")\n",
    "    print(f\"   Weights: Age={weight_age:.2f}, Income={weight_income:.3f}\")\n",
    "    print(f\"   Bias: {bias:.2f}\")\n",
    "\n",
    "    if accuracy >= 0.8:\n",
    "        print(\"\\n🎉 Excellent! Your neuron learned the pattern well!\")\n",
    "    elif accuracy >= 0.6:\n",
    "        print(\"\\n✅ Good! Try adjusting weights for better performance.\")\n",
    "    else:\n",
    "        print(\"\\n🤔 Keep experimenting with the weights and bias!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What This Does:\n",
    "\n",
    "- It shows you how well your weights perform.\n",
    "- The first plot shows which predictions were right (green) or wrong (red).\n",
    "- The second plot shows your neuron’s decision boundary: where it predicts “yes” vs “no”.\n",
    "\n",
    "### Interpreting the Output:\n",
    "\n",
    "- A boundary that cuts through the data well means your neuron learned something useful!\n",
    "- A boundary that misses many points means the neuron is too simple — or poorly tuned.\n",
    "- Look at the accuracy printed — aim for at least 80%!\n",
    "\n",
    "🧠 This helps you build intuition for:\n",
    "- What weights and bias do\n",
    "- How decision boundaries work\n",
    "- Why real neural networks need more neurons and more layers\n",
    "\n",
    "### 💡 Key Takeaways\n",
    "\n",
    "- A single neuron is powerful but limited.\n",
    "- It struggles with curved or complex boundaries.\n",
    "- You just discovered why we need multi-layer neural networks — to handle complex patterns in data!\n",
    "\n",
    "👉 Next, we’ll move from one neuron to an actual network, and see how it improves performance dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building Your First Neural Network\n",
    "\n",
    "### 🔄 From a Single Neuron to a Learning Machine\n",
    "\n",
    "A single neuron can draw a line — but it can’t capture curves, corners, or complex decision boundaries.  \n",
    "To move beyond that, we need **many neurons** working together in **layers**.\n",
    "\n",
    "This is the core idea of a **neural network**.\n",
    "\n",
    "### 🧱 What Happens Inside a Neural Network?\n",
    "\n",
    "- The **input layer** passes features to the network\n",
    "- **Hidden layers** process information step by step:\n",
    "  - The first layer might detect simple signals\n",
    "  - The next layer combines them into patterns\n",
    "  - Later layers build more abstract understanding\n",
    "- The **output layer** makes the final prediction\n",
    "\n",
    "### 🌟 Why Is This Powerful?\n",
    "\n",
    "- **Multiple neurons** allow flexible, curved decision boundaries  \n",
    "- **Multiple layers** allow abstraction: from pixels → edges → shapes → faces  \n",
    "- **Activation functions** give the network its non-linear magic\n",
    "\n",
    "With the right architecture, a neural network can learn just about anything.\n",
    "\n",
    "Let’s build one!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌀 Step 1: Create a Challenging Dataset\n",
    "\n",
    "We’ll now create a dataset that **cannot be solved with straight lines**.  \n",
    "It has circular patterns — something only neural networks can untangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex dataset that needs a neural network\n",
    "X, y = make_circles(n_samples=300, noise=0.1, factor=0.3, random_state=42)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "circle_data = pd.DataFrame(X, columns=['x', 'y'])\n",
    "circle_data['class'] = y\n",
    "\n",
    "print(\"🎯 Complex Pattern Dataset:\")\n",
    "print(f\"   Data points: {len(circle_data)}\")\n",
    "print(f\"   Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 Step 2: Visualize the Problem\n",
    "\n",
    "Let’s plot the data to see why this is tricky.  \n",
    "The two classes form **concentric circles** — a single line won’t separate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the complex pattern\n",
    "fig = px.scatter(circle_data, x='x', y='y', color='class',\n",
    "                color_discrete_map={0: 'red', 1: 'blue'},\n",
    "                title=\"🎯 Complex Pattern: Circles within Circles\")\n",
    "\n",
    "fig.update_layout(height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🧠 **Observation**:  \n",
    "- ❌ **Linear models** will fail here  \n",
    "- ❌ **Single neurons** won’t help  \n",
    "- ✅ **Neural networks** are up for the challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 🛠️ Step 3: Prepare the Data\n",
    "\n",
    "We’ll now split the data and scale it.  \n",
    "**Why scale?** Neural networks are sensitive to the scale of input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the data (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤖 Step 4: Build and Train a Neural Network\n",
    "\n",
    "Let’s create a real neural network with:\n",
    "- **2 hidden layers**\n",
    "- **10 neurons each**\n",
    "- **ReLU activation**\n",
    "- Up to **1000 training iterations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network with Keras\n",
    "model = keras.Sequential([\n",
    "    Input(shape=(2,)),\n",
    "    layers.Dense(10, activation='relu'),\n",
    "    layers.Dense(10, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model and store training history\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, verbose=0, validation_data=(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📈 Step 5: Evaluate Performance\n",
    "\n",
    "Now let’s test the network’s accuracy and see how well it learned the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training and test sets\n",
    "train_loss, train_accuracy = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "# Report results\n",
    "print(f\"\\n🧠 Neural Network Results:\")\n",
    "print(f\"   Training Accuracy: {train_accuracy:.1%}\")\n",
    "print(f\"   Test Accuracy: {test_accuracy:.1%}\")\n",
    "print(f\"   Network Architecture: Input → 10 → 10 → Output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎉 Great job! You've just trained a real neural network on a dataset that would stump simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 Step 7: Visualizing the Neural Network's Performance\n",
    "\n",
    "Now that we've trained our neural network using TensorFlow/Keras, let’s see what it actually learned.\n",
    "\n",
    "We'll look at:\n",
    "- The decision boundary: where the model predicts one class vs another\n",
    "- The loss curves: how the model improved during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create meshgrid for decision boundary\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Predict over the grid\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_points_scaled = scaler.transform(grid_points)\n",
    "Z = model.predict(grid_points_scaled, verbose=0)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Neural Network Decision Boundary', 'Training Loss Curve'),\n",
    "    specs=[[{\"type\": \"contour\"}, {\"type\": \"xy\"}]]\n",
    ")\n",
    "\n",
    "# Subplot 1: Decision boundary\n",
    "fig.add_trace(\n",
    "    go.Contour(\n",
    "        x=np.arange(x_min, x_max, h),\n",
    "        y=np.arange(y_min, y_max, h),\n",
    "        z=Z,\n",
    "        colorscale='RdYlBu',\n",
    "        showscale=False,\n",
    "        contours=dict(start=0, end=1, size=0.05),\n",
    "        opacity=0.6,\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "point_colors = ['red' if label == 0 else 'blue' for label in y]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=X[:, 0], y=X[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            color=point_colors,\n",
    "            line=dict(width=1, color='black'),\n",
    "            size=8\n",
    "        ),\n",
    "        name='Data Points'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Subplot 2: Training and validation loss\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(1, len(history.history['loss']) + 1)),\n",
    "        y=history.history['loss'],\n",
    "        mode='lines',\n",
    "        name='Training Loss',\n",
    "        line=dict(width=3, color='green')\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(1, len(history.history['val_loss']) + 1)),\n",
    "        y=history.history['val_loss'],\n",
    "        mode='lines',\n",
    "        name='Validation Loss',\n",
    "        line=dict(width=3, dash='dash', color='blue')\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Layout\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    title_text=f\"🧠 Neural Network: Decision Boundary and Loss Curve (Test Accuracy: {test_accuracy:.1%})\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"X1\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"X2\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Epoch\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Loss\", row=1, col=2)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💬 What Can We Conclude?\n",
    "\n",
    "- The **decision boundary** shows that the neural network learned to separate the circular classes quite well — something **linear models could never do**.\n",
    "- The **loss curves** confirm that the model was able to reduce error during training and generalize well to unseen data (validation loss stays low).\n",
    "\n",
    "Neural networks **automatically learn complex patterns** through multiple layers and nonlinear activations — that’s their power!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎮 Explore Neural Networks Visually (Optional)\n",
    "\n",
    "Want to **play with neurons, layers, and activations** without writing code?\n",
    "\n",
    "👉 Head over to the [**TensorFlow Playground**](https://playground.tensorflow.org/)!\n",
    "\n",
    "You can:\n",
    "- Add or remove hidden layers\n",
    "- Try different activation functions\n",
    "- Tune learning rates and regularization\n",
    "- Watch how the network adjusts to complex patterns like **concentric circles**\n",
    "\n",
    "🔍 **Tip:** In the Playground, use the dataset with two circles (bottom-right option).  \n",
    "It's the same one we just trained on!\n",
    "\n",
    "This interactive tool is a perfect way to **see** what neural networks are learning — in real time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🖼️ Deep Learning in Action: Recognizing Handwritten Digits\n",
    "\n",
    "We’ve seen how neural networks can learn curved shapes — but can they recognize **images**?\n",
    "\n",
    "In this example, we’ll use real image data:\n",
    "- Each image is just **8×8 grayscale pixels**\n",
    "- Each one shows a handwritten **digit (0–9)**\n",
    "- Your goal: build a deep neural network to **recognize digits automatically**\n",
    "\n",
    "This is a **mini-version of image recognition** and shows how deep learning powers tasks like OCR (optical character recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X_digits, y_digits = digits.data, digits.target\n",
    "\n",
    "print(\"🖼️ Handwritten Digit Recognition Dataset:\")\n",
    "print(f\"   Images: {len(X_digits)}\")\n",
    "print(f\"   Pixels per image: {X_digits.shape[1]} (8x8 grid)\")\n",
    "print(f\"   Classes: {len(set(y_digits))} digits (0–9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 👁️‍🗨️ Let’s Look at the Data\n",
    "\n",
    "Each digit is represented by a flat list of 64 numbers (8×8 grayscale pixels). Let’s visualize some of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example digits\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=5,\n",
    "    subplot_titles=[f'Digit {i}' for i in range(10)]\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    idx = (y_digits == i).nonzero()[0][0]\n",
    "    image = X_digits[idx].reshape(8, 8)\n",
    "\n",
    "    row = 1 if i < 5 else 2\n",
    "    col = (i % 5) + 1\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=image, colorscale='gray', showscale=False),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"🖼️ Sample Handwritten Digits (8x8 pixels)\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 Step 1: Prepare the Data\n",
    "\n",
    "We scale the pixel values (just like with the circles) and split the data into training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and scale the data\n",
    "X_train_digits, X_test_digits, y_train_digits, y_test_digits = train_test_split(\n",
    "    X_digits, y_digits, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler_digits = StandardScaler()\n",
    "X_train_digits_scaled = scaler_digits.fit_transform(X_train_digits)\n",
    "X_test_digits_scaled = scaler_digits.transform(X_test_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤖 Step 2: Build a Deep Neural Network\n",
    "\n",
    "We now build a **deeper network** that can:\n",
    "- Learn from all 64 input pixels\n",
    "- Use **ReLU activations**\n",
    "- Have **3 hidden layers** with decreasing size (100 → 50 → 25)\n",
    "- Predict the correct digit (0–9) using a **softmax output layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_digits = keras.Sequential([\n",
    "    layers.Input(shape=(64,)),  # 8x8 images flattened to 64 pixels\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(25, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # 10 output classes\n",
    "])\n",
    "\n",
    "model_digits.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_digits = model_digits.fit(\n",
    "    X_train_digits_scaled, y_train_digits,\n",
    "    epochs=30, validation_data=(X_test_digits_scaled, y_test_digits),\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📈 Step 3: Check Accuracy\n",
    "\n",
    "Let’s see how well the model performs on both training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = model_digits.evaluate(X_train_digits_scaled, y_train_digits, verbose=0)\n",
    "test_loss, test_acc = model_digits.evaluate(X_test_digits_scaled, y_test_digits, verbose=0)\n",
    "\n",
    "print(f\"🎯 Deep Learning Results:\")\n",
    "print(f\"   Training Accuracy: {train_acc:.1%}\")\n",
    "print(f\"   Test Accuracy: {test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 Step 4: Try the Network on New Digits\n",
    "\n",
    "Let’s ask the model to classify some unseen digits — and show how confident it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 5 random digits from the test set\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(X_test_digits), size=5, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    true_label = y_test_digits[idx]\n",
    "    image = X_test_digits[idx].reshape(1, -1)\n",
    "    prediction = model_digits.predict(image, verbose=0)\n",
    "    predicted_label = np.argmax(prediction)\n",
    "    confidence = np.max(prediction)\n",
    "\n",
    "    print(f\"🔢 True: {true_label} | Predicted: {predicted_label} | Confidence: {confidence:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Summary: What You Learned\n",
    "\n",
    "- Neural networks can go **beyond synthetic patterns** and handle real-world images.\n",
    "- With enough depth and training, they can classify digits with **over 95% accuracy**!\n",
    "- This is a simplified version of what powers real tools like OCR, CAPTCHA solvers, and postal scanners.\n",
    "\n",
    "Next steps:\n",
    "- Learn how **convolutional layers** can improve image recognition\n",
    "- Explore more complex datasets like **MNIST** or **CIFAR-10**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎮 Try It Yourself: Digit Classifier in Action\n",
    "\n",
    "Use the slider below to explore how the neural network performs on different handwritten digits.\n",
    "\n",
    "Watch:\n",
    "- What it gets **right** ✅\n",
    "- What it **misclassifies** ❌\n",
    "- How **confident** it is in its answers 🔍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(\n",
    "    digit_index=IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=len(X_test_digits) - 1,\n",
    "        step=1,\n",
    "        description='Digit #:',\n",
    "        continuous_update=False\n",
    "    )\n",
    ")\n",
    "def test_digit_classifier_keras(digit_index):\n",
    "    \"\"\"Interactive digit classification tool using Keras model\"\"\"\n",
    "\n",
    "    # Safeguard against out-of-bounds index\n",
    "    if digit_index >= len(X_test_digits):\n",
    "        digit_index = len(X_test_digits) - 1\n",
    "\n",
    "    test_image = X_test_digits[digit_index]\n",
    "    true_label = y_test_digits[digit_index]\n",
    "\n",
    "    # Predict with Keras model\n",
    "    input_scaled = X_test_digits_scaled[digit_index].reshape(1, -1)\n",
    "    probabilities = model_digits.predict(input_scaled, verbose=0)[0]\n",
    "    prediction = np.argmax(probabilities)\n",
    "\n",
    "    # Create side-by-side plots\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('🖼️ Test Image', '📊 Network Prediction'),\n",
    "        specs=[[{\"type\": \"heatmap\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "\n",
    "    # Left: Show the digit image\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=test_image.reshape(8, 8), colorscale='gray', showscale=False),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Right: Bar chart of predicted probabilities\n",
    "    digits = list(range(10))\n",
    "    bar_colors = ['green' if i == prediction else 'lightblue' for i in digits]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=digits, y=probabilities, marker_color=bar_colors),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # Add vertical line for the true label\n",
    "    fig.add_vline(x=true_label, line_dash=\"dash\", line_color=\"red\",\n",
    "                  annotation_text=f\"True: {true_label}\", row=1, col=2)\n",
    "\n",
    "    # Prepare summary\n",
    "    result = \"✅ CORRECT\" if prediction == true_label else \"❌ WRONG\"\n",
    "    confidence = probabilities[prediction]\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=400,\n",
    "        title_text=f\"🧠 Prediction: {prediction} (True: {true_label}) — {result} ({confidence:.1%} confidence)\"\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Pixel Position\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Digit\", row=1, col=2, tickmode='linear', dtick=1)\n",
    "    fig.update_yaxes(title_text=\"Pixel Position\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Probability\", row=1, col=2)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    print(f\"🎯 Classification Summary:\")\n",
    "    print(f\"   True Digit: {true_label}\")\n",
    "    print(f\"   Predicted : {prediction}\")\n",
    "    print(f\"   Confidence: {confidence:.1%}\")\n",
    "    print(f\"   Result    : {result}\")\n",
    "\n",
    "    top_3 = np.argsort(probabilities)[-3:][::-1]\n",
    "    print(f\"\\n🏆 Top 3 Predictions:\")\n",
    "    for rank, idx in enumerate(top_3, start=1):\n",
    "        print(f\"   {rank}. Digit {idx}: {probabilities[idx]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Bonus: Neural Networks in PyTorch\n",
    "\n",
    "Keras (TensorFlow) is great for fast prototyping — but PyTorch offers flexibility and is widely used in research.\n",
    "\n",
    "Here’s how you can build and train the same neural network using **PyTorch**.\n",
    "\n",
    "We'll:\n",
    "- Define a custom neural network class\n",
    "- Use binary cross-entropy loss and the Adam optimizer\n",
    "- Train the model and evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Instantiate the model, define loss and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = model(X_test_tensor).numpy()\n",
    "    test_preds_class = (test_preds > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, test_preds_class)\n",
    "\n",
    "print(f\"✅ PyTorch Test Accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔄 What’s Different?\n",
    "\n",
    "- You define your own `nn.Module` class to specify the architecture\n",
    "- Training uses a manual loop (gives you more control)\n",
    "- PyTorch works well with **autograd** and **GPU acceleration**\n",
    "\n",
    "Both Keras and PyTorch are excellent — the choice often depends on the task and your preference.\n",
    "\n",
    "🧪 Try recreating this network for other datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Congratulations – You've Completed the Machine Learning Series!\n",
    "\n",
    "Over the past three sessions, you've built a strong foundation in machine learning – from simple models to deep neural networks powering modern AI.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧭 Your Learning Journey\n",
    "\n",
    "**🔹 Class 1: Linear Models**\n",
    "- Discovered how machines learn from data with straight-line models  \n",
    "- Understood gradient descent as a tool for optimization  \n",
    "- Built the mathematical intuition behind model training  \n",
    "\n",
    "**🔹 Class 2: Decision Trees & Ensembles**  \n",
    "- Learned how models split data into interpretable rules  \n",
    "- Saw how combining many trees creates powerful predictors (Random Forests)  \n",
    "- Gained tools that balance accuracy and explainability  \n",
    "\n",
    "**🔹 Class 3: Neural Networks & Deep Learning**  \n",
    "- Explored how layered “neurons” can learn complex, non-linear patterns  \n",
    "- Trained deep models to recognize patterns in visuals and data  \n",
    "- Saw how AI powers modern tools like voice assistants, image recognition, and language translation  \n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Big Takeaways\n",
    "\n",
    "- 🧠 **No single model fits all problems** – know your tools  \n",
    "- 🔍 **Complex patterns require complex models**, but they also need more data and care  \n",
    "- ⚖️ **Simplicity vs. performance is always a trade-off**  \n",
    "- 🔄 **Interpreting and validating results is as important as building the model**\n",
    "\n",
    "---\n",
    "\n",
    "### 🌐 Where Machine Learning Shows Up Around You\n",
    "\n",
    "- 🎬 **Netflix recommendations** – collaborative filtering + neural nets  \n",
    "- 📸 **Face recognition** – deep convolutional networks  \n",
    "- ✉️ **Spam detection** – decision trees on text features  \n",
    "- 🚗 **Self-driving perception** – computer vision + reinforcement learning  \n",
    "- 🌍 **Real-time translation** – sequence models and transformers  \n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 What's Next?\n",
    "\n",
    "**🔧 Practice & Projects**\n",
    "- Apply what you’ve learned to datasets from your field  \n",
    "- Build your first real-world ML mini-project  \n",
    "- Use platforms like Kaggle or Hugging Face to explore datasets and models  \n",
    "\n",
    "**📚 Keep Learning**\n",
    "- Dive into:\n",
    "  - Computer Vision\n",
    "  - Natural Language Processing\n",
    "  - Reinforcement Learning\n",
    "  - Generative AI  \n",
    "- Try TensorFlow, PyTorch, Scikit-Learn in more depth\n",
    "\n",
    "💼 **Start using machine learning in your field!**\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ Final Wisdom\n",
    "\n",
    "You don’t need a PhD to start using machine learning effectively.\n",
    "\n",
    "What matters most:\n",
    "- 🔍 **Knowing which model fits the problem**\n",
    "- 🧪 **Testing, validating, and improving your model**\n",
    "- 🧠 **Thinking critically about data and bias**\n",
    "- 📢 **Explaining your results to others**\n",
    "\n",
    "---\n",
    "\n",
    "### 🌟 You’re Ready\n",
    "\n",
    "This is just the beginning — ML is a journey of curiosity, experimentation, and creativity.\n",
    "\n",
    "Go explore, build, ask questions, and make something amazing.  \n",
    "We’re excited to see what you’ll do. 🚀🤖\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDV-ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
