{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning Class 3: Neural Networks & Deep Learning\n",
    "\n",
    "Welcome to the third and final class in our machine learning journey!\n",
    "So far, weâ€™ve learned how:\n",
    "\n",
    "- Linear models uncover simple trends in data ğŸ“ˆ\n",
    "- Decision trees ask smart questions to classify and predict ğŸŒ³\n",
    "\n",
    "Today, we turn to the most powerful tool in modern AI:\n",
    "\n",
    "### âœ¨ Neural Networks â€” systems inspired by the brain that can learn almost anything.\n",
    "\n",
    "### ğŸ¯ What You'll Learn Today\n",
    "\n",
    "1. **Neural Network Basics** â€” How artificial neurons learn from data\n",
    "2. **Building Your First Network** â€” Step-by-step hands-on demo\n",
    "3. **Going Deeper** â€” Why â€œdeepâ€ learning is so powerful\n",
    "4. **Real Applications** â€” From image recognition to language translation\n",
    "5. **Bonus** â€” Learn how to build the same network in Keras and PyTorch\n",
    "\n",
    "### ğŸ§  **The Core Idea**\n",
    "\n",
    "Neural networks are made of layers of tiny computing units â€” neurons â€” that:\n",
    "\n",
    "- Take inputs (like pixel values or text features)\n",
    "- Apply weights and nonlinear activations\n",
    "- Pass results forward to make predictions\n",
    "\n",
    "With enough neurons and layers, these networks can:\n",
    "\n",
    "- Recognize complex images ğŸ–¼ï¸\n",
    "- Understand language and context ğŸŒ\n",
    "- Learn intricate patterns that no human could code by hand\n",
    "\n",
    "### ğŸ”— Building on Previous Classes\n",
    "\n",
    "| Class | Focus | Key Idea |\n",
    "|-------|-------|----------|\n",
    "| 1ï¸âƒ£ Linear Models | Fit lines to patterns | Models assume a simple structure |\n",
    "| 2ï¸âƒ£ Trees | Ask yes/no questions | Let data dictate structure |\n",
    "| 3ï¸âƒ£ Neural Nets | Learn anything | Model learns structure from scratch |\n",
    "\n",
    "Neural networks **donâ€™t need hand-coded rules**. They build their own understanding by adjusting internal parameters based on data â€” making them the foundation of everything from ChatGPT to self-driving cars.\n",
    "\n",
    "### ğŸŒ Why Neural Networks Matter\n",
    "\n",
    "They power many of the tools you use daily:\n",
    "\n",
    "- ğŸ“¸ Image classifiers on your phone\n",
    "- ğŸ§  Large Language Models like ChatGPT\n",
    "- ğŸ§ Music & movie recommendations\n",
    "- ğŸš— Autonomous vehicles\n",
    "- ğŸ§¬ Healthcare diagnostics\n",
    "\n",
    "And theyâ€™re just getting started.\n",
    "\n",
    "---\n",
    "\n",
    "Letâ€™s dive in and build one from scratch â€” then see how to train it using real-world libraries like TensorFlow/Keras and PyTorch.\n",
    "\n",
    "Get ready to enter the world of deep learning! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Google Colab, clone the repo (if needed),\n",
    "# move into the repo directory, and ensure itâ€™s on the Python path.\n",
    "\n",
    "import sys, os\n",
    "\n",
    "def in_colab():\n",
    "    try: import google.colab; return True\n",
    "    except: return False\n",
    "\n",
    "if in_colab():\n",
    "    repo = \"Hands-On-Notebooks\"\n",
    "    if os.path.basename(os.getcwd()) != repo:\n",
    "        if not os.path.exists(repo):\n",
    "            !git clone https://github.com/BridgingAISocietySummerSchools/{repo}\n",
    "        %cd {repo}\n",
    "    if '.' not in sys.path:\n",
    "        sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Setup - Import Our Neural Network Tools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_circles, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Input\n",
    "\n",
    "from plotting_utils.neural_nets import (\n",
    "    plot_customer_data_scatter,\n",
    "    create_interactive_neuron_designer,\n",
    "    plot_circle_data_scatter,\n",
    "    plot_neural_network_results,\n",
    "    plot_digit_samples,\n",
    "    create_interactive_digit_classifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducible results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Neural Networks - Artificial Brains\n",
    "\n",
    "### ğŸ§  **How Does Your Brain Work?**\n",
    "\n",
    "Your brain has ~86 billion neurons that:\n",
    "1. **Receive signals** from other neurons\n",
    "2. **Process information** by combining signals\n",
    "3. **Send output** to other neurons if activated\n",
    "4. **Learn** by strengthening important connections\n",
    "\n",
    "**Artificial neural networks** are a *simplified version* of this idea:\n",
    "\n",
    "- They use numbers instead of electric signals.\n",
    "- They \"learn\" by changing weights through data and feedback.\n",
    "\n",
    "### ğŸ”— **From Biological to Artificial**\n",
    "\n",
    "Let's break down what a single **artificial neuron** does:\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Inputs** | Numbers representing features (like age, income, etc.) |\n",
    "| **Weights** | How important each input is (learned during training) |\n",
    "| **Activation** | Decides whether to \"fire\" based on weighted inputs |\n",
    "| **Output** | A number passed to the next layer |\n",
    "\n",
    "A **neural network** simply connects many of these neurons in layers:\n",
    "\n",
    "- **Input layer** takes your data\n",
    "- **Hidden layers** do the processing\n",
    "- **Output layer** gives the final prediction\n",
    "\n",
    "### ğŸ¯ Why Networks Beat Single Neurons\n",
    "\n",
    "- A single neuron can only draw a straight line â€” itâ€™s like linear regression.\n",
    "- A network with more neurons and layers can:\n",
    "  - Bend, curve, and twist boundaries ğŸŒ€\n",
    "  - Combine patterns from multiple inputs\n",
    "  - Capture non-linear structures in data\n",
    "\n",
    "Letâ€™s build some intuition with a real example. ğŸ“Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Example: Can a Single Neuron Predict Who Will Buy?\n",
    "\n",
    "Weâ€™ll use a tiny **customer dataset** with just two inputs: `age` and `income`.\n",
    "\n",
    "Each customer either bought a product (1) or did not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data = pd.DataFrame({\n",
    "    'age': [25, 35, 45, 55, 30, 40, 50, 28, 38, 48],\n",
    "    'income': [30, 50, 70, 90, 40, 60, 80, 35, 55, 75],\n",
    "    'will_buy': [0, 0, 1, 1, 0, 1, 1, 0, 0, 1]\n",
    "})\n",
    "print(customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ˆ Let's Visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_customer_data_scatter(customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª Testing a Single Neuron\n",
    "\n",
    "Letâ€™s simulate how one artificial neuron would handle a single customer. Let's assume the custom is 40 years old with an income of $60,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_neuron(age, income, weight_age=0.1, weight_income=0.05, bias=-3):\n",
    "    weighted_sum = weight_age * age + weight_income * income + bias\n",
    "\n",
    "    # Let's use a sigmoid activation function\n",
    "    activation = 1 / (1 + np.exp(-weighted_sum))\n",
    "    prediction = 1 if activation > 0.5 else 0\n",
    "\n",
    "    return prediction, activation\n",
    "\n",
    "prediction, confidence = simple_neuron(age=40, income=60)\n",
    "print(f\"Prediction: {'Will Buy' if prediction == 1 else 'Will Not Buy'}, Confidence: {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ® Interactive: Design Your Own Neuron\n",
    "\n",
    "The following code defines a helper function to visualize how a single artificial neuron makes predictions.  \n",
    "Donâ€™t worry if it looks long â€” it's just setting up the plotting and interaction.\n",
    "\n",
    "Now, use the sliders below to adjust the weights and bias and see how your neuron performs!  \n",
    "Can you find a setting that gets most predictions right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_interactive_neuron_designer(customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What This Does:\n",
    "\n",
    "- It shows you how well your weights perform.\n",
    "- The first plot shows which predictions were right (green) or wrong (red).\n",
    "- The second plot shows your neuronâ€™s decision boundary: where it predicts â€œyesâ€ vs â€œnoâ€.\n",
    "\n",
    "### Interpreting the Output:\n",
    "\n",
    "- A boundary that cuts through the data well means your neuron learned something useful!\n",
    "- A boundary that misses many points means the neuron is too simple â€” or poorly tuned.\n",
    "- Look at the accuracy printed â€” aim for at least 80%!\n",
    "\n",
    "ğŸ§  This helps you build intuition for:\n",
    "- What weights and bias do\n",
    "- How decision boundaries work\n",
    "- Why real neural networks need more neurons and more layers\n",
    "\n",
    "### ğŸ’¡ Key Takeaways\n",
    "\n",
    "- A single neuron is powerful but limited.\n",
    "- It struggles with curved or complex boundaries.\n",
    "- You just discovered why we need multi-layer neural networks â€” to handle complex patterns in data!\n",
    "\n",
    "ğŸ‘‰ Next, weâ€™ll move from one neuron to an actual network, and see how it improves performance dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Building Your First Neural Network\n",
    "\n",
    "### ğŸ”„ From a Single Neuron to a Learning Machine\n",
    "\n",
    "A single neuron can draw a line â€” but it canâ€™t capture curves, corners, or complex decision boundaries.  \n",
    "To move beyond that, we need **many neurons** working together in **layers**.\n",
    "\n",
    "This is the core idea of a **neural network**.\n",
    "\n",
    "### ğŸ§± What Happens Inside a Neural Network?\n",
    "\n",
    "- The **input layer** passes features to the network\n",
    "- **Hidden layers** process information step by step:\n",
    "  - The first layer might detect simple signals\n",
    "  - The next layer combines them into patterns\n",
    "  - Later layers build more abstract understanding\n",
    "- The **output layer** makes the final prediction\n",
    "\n",
    "### ğŸŒŸ Why Is This Powerful?\n",
    "\n",
    "- **Multiple neurons** allow flexible, curved decision boundaries  \n",
    "- **Multiple layers** allow abstraction: from pixels â†’ edges â†’ shapes â†’ faces  \n",
    "- **Activation functions** give the network its non-linear magic\n",
    "\n",
    "With the right architecture, a neural network can learn just about anything.\n",
    "\n",
    "Letâ€™s build one!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸŒ€ Step 1: Create a Challenging Dataset\n",
    "\n",
    "Weâ€™ll now create a dataset that **cannot be solved with straight lines**.  \n",
    "It has circular patterns â€” something only neural networks can untangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex dataset that needs a neural network\n",
    "X, y = make_circles(n_samples=300, noise=0.1, factor=0.3, random_state=42)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "circle_data = pd.DataFrame(X, columns=['x', 'y'])\n",
    "circle_data['class'] = y\n",
    "\n",
    "print(\"ğŸ¯ Complex Pattern Dataset:\")\n",
    "print(f\"   Data points: {len(circle_data)}\")\n",
    "print(f\"   Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Step 2: Visualize the Problem\n",
    "\n",
    "Letâ€™s plot the data to see why this is tricky.  \n",
    "The two classes form **concentric circles** â€” a single line wonâ€™t separate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_circle_data_scatter(circle_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ§  **Observation**:  \n",
    "- âŒ **Linear models** will fail here  \n",
    "- âŒ **Single neurons** wonâ€™t help  \n",
    "- âœ… **Neural networks** are up for the challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ğŸ› ï¸ Step 3: Prepare the Data\n",
    "\n",
    "Weâ€™ll now split the data and scale it.  \n",
    "**Why scale?** Neural networks are sensitive to the scale of input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the data (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤– Step 4: Build and Train a Neural Network\n",
    "\n",
    "Letâ€™s create a real neural network with:\n",
    "- **2 hidden layers**\n",
    "- **10 neurons each**\n",
    "- **ReLU activation**\n",
    "- Up to **1000 training iterations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network with Keras\n",
    "model = keras.Sequential([\n",
    "    Input(shape=(2,)),\n",
    "    layers.Dense(10, activation='relu'),\n",
    "    layers.Dense(10, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model and store training history\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, verbose=0, validation_data=(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ˆ Step 5: Evaluate Performance\n",
    "\n",
    "Now letâ€™s test the networkâ€™s accuracy and see how well it learned the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training and test sets\n",
    "train_loss, train_accuracy = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "# Report results\n",
    "print(f\"\\nğŸ§  Neural Network Results:\")\n",
    "print(f\"   Training Accuracy: {train_accuracy:.1%}\")\n",
    "print(f\"   Test Accuracy: {test_accuracy:.1%}\")\n",
    "print(f\"   Network Architecture: Input â†’ 10 â†’ 10 â†’ Output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ‰ Great job! You've just trained a real neural network on a dataset that would stump simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Step 7: Visualizing the Neural Network's Performance\n",
    "\n",
    "Now that we've trained our neural network using TensorFlow/Keras, letâ€™s see what it actually learned.\n",
    "\n",
    "We'll look at:\n",
    "- The decision boundary: where the model predicts one class vs another\n",
    "- The loss curves: how the model improved during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_neural_network_results(X, y, model, scaler, history, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¬ What Can We Conclude?\n",
    "\n",
    "- The **decision boundary** shows that the neural network learned to separate the circular classes quite well â€” something **linear models could never do**.\n",
    "- The **loss curves** confirm that the model was able to reduce error during training and generalize well to unseen data (validation loss stays low).\n",
    "\n",
    "Neural networks **automatically learn complex patterns** through multiple layers and nonlinear activations â€” thatâ€™s their power!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ® Explore Neural Networks Visually (Optional)\n",
    "\n",
    "Want to **play with neurons, layers, and activations** without writing code?\n",
    "\n",
    "ğŸ‘‰ Head over to the [**TensorFlow Playground**](https://playground.tensorflow.org/)!\n",
    "\n",
    "You can:\n",
    "- Add or remove hidden layers\n",
    "- Try different activation functions\n",
    "- Tune learning rates and regularization\n",
    "- Watch how the network adjusts to complex patterns like **concentric circles**\n",
    "\n",
    "ğŸ” **Tip:** In the Playground, use the dataset with two circles (bottom-right option).  \n",
    "It's the same one we just trained on!\n",
    "\n",
    "This interactive tool is a perfect way to **see** what neural networks are learning â€” in real time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ–¼ï¸ Deep Learning in Action: Recognizing Handwritten Digits\n",
    "\n",
    "Weâ€™ve seen how neural networks can learn curved shapes â€” but can they recognize **images**?\n",
    "\n",
    "In this example, weâ€™ll use real image data:\n",
    "- Each image is just **8Ã—8 grayscale pixels**\n",
    "- Each one shows a handwritten **digit (0â€“9)**\n",
    "- Your goal: build a deep neural network to **recognize digits automatically**\n",
    "\n",
    "This is a **mini-version of image recognition** and shows how deep learning powers tasks like OCR (optical character recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X_digits, y_digits = digits.data, digits.target\n",
    "\n",
    "print(\"ğŸ–¼ï¸ Handwritten Digit Recognition Dataset:\")\n",
    "print(f\"   Images: {len(X_digits)}\")\n",
    "print(f\"   Pixels per image: {X_digits.shape[1]} (8x8 grid)\")\n",
    "print(f\"   Classes: {len(set(y_digits))} digits (0â€“9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‘ï¸â€ğŸ—¨ï¸ Letâ€™s Look at the Data\n",
    "\n",
    "Each digit is represented by a flat list of 64 numbers (8Ã—8 grayscale pixels). Letâ€™s visualize some of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digit_samples(X_digits, y_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§  Step 1: Prepare the Data\n",
    "\n",
    "We scale the pixel values (just like with the circles) and split the data into training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and scale the data\n",
    "X_train_digits, X_test_digits, y_train_digits, y_test_digits = train_test_split(\n",
    "    X_digits, y_digits, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler_digits = StandardScaler()\n",
    "X_train_digits_scaled = scaler_digits.fit_transform(X_train_digits)\n",
    "X_test_digits_scaled = scaler_digits.transform(X_test_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤– Step 2: Build a Deep Neural Network\n",
    "\n",
    "We now build a **deeper network** that can:\n",
    "- Learn from all 64 input pixels\n",
    "- Use **ReLU activations**\n",
    "- Have **3 hidden layers** with decreasing size (100 â†’ 50 â†’ 25)\n",
    "- Predict the correct digit (0â€“9) using a **softmax output layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_digits = keras.Sequential([\n",
    "    layers.Input(shape=(64,)),  # 8x8 images flattened to 64 pixels\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(25, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # 10 output classes\n",
    "])\n",
    "\n",
    "model_digits.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_digits = model_digits.fit(\n",
    "    X_train_digits_scaled, y_train_digits,\n",
    "    epochs=30, validation_data=(X_test_digits_scaled, y_test_digits),\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ˆ Step 3: Check Accuracy\n",
    "\n",
    "Letâ€™s see how well the model performs on both training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = model_digits.evaluate(X_train_digits_scaled, y_train_digits, verbose=0)\n",
    "test_loss, test_acc = model_digits.evaluate(X_test_digits_scaled, y_test_digits, verbose=0)\n",
    "\n",
    "print(f\"ğŸ¯ Deep Learning Results:\")\n",
    "print(f\"   Training Accuracy: {train_acc:.1%}\")\n",
    "print(f\"   Test Accuracy: {test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Step 4: Try the Network on New Digits\n",
    "\n",
    "Letâ€™s ask the model to classify some unseen digits â€” and show how confident it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 5 random digits from the test set\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(X_test_digits), size=5, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    true_label = y_test_digits[idx]\n",
    "    image = X_test_digits[idx].reshape(1, -1)\n",
    "    prediction = model_digits.predict(image, verbose=0)\n",
    "    predicted_label = np.argmax(prediction)\n",
    "    confidence = np.max(prediction)\n",
    "\n",
    "    print(f\"ğŸ”¢ True: {true_label} | Predicted: {predicted_label} | Confidence: {confidence:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Summary: What You Learned\n",
    "\n",
    "- Neural networks can go **beyond synthetic patterns** and handle real-world images.\n",
    "- With enough depth and training, they can classify digits with **over 95% accuracy**!\n",
    "- This is a simplified version of what powers real tools like OCR, CAPTCHA solvers, and postal scanners.\n",
    "\n",
    "Next steps:\n",
    "- Learn how **convolutional layers** can improve image recognition\n",
    "- Explore more complex datasets like **MNIST** or **CIFAR-10**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ® Try It Yourself: Digit Classifier in Action\n",
    "\n",
    "Use the slider below to explore how the neural network performs on different handwritten digits.\n",
    "\n",
    "Watch:\n",
    "- What it gets **right** âœ…\n",
    "- What it **misclassifies** âŒ\n",
    "- How **confident** it is in its answers ğŸ”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_interactive_digit_classifier(X_test_digits, y_test_digits, X_test_digits_scaled, model_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Bonus: Neural Networks in PyTorch\n",
    "\n",
    "Keras (TensorFlow) is great for fast prototyping â€” but PyTorch offers flexibility and is widely used in research.\n",
    "\n",
    "Hereâ€™s how you can build and train the same neural network using **PyTorch**.\n",
    "\n",
    "We'll:\n",
    "- Define a custom neural network class\n",
    "- Use binary cross-entropy loss and the Adam optimizer\n",
    "- Train the model and evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Instantiate the model, define loss and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate on test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = model(X_test_tensor).numpy()\n",
    "    test_preds_class = (test_preds > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, test_preds_class)\n",
    "\n",
    "print(f\"âœ… PyTorch Test Accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”„ Whatâ€™s Different?\n",
    "\n",
    "- You define your own `nn.Module` class to specify the architecture\n",
    "- Training uses a manual loop (gives you more control)\n",
    "- PyTorch works well with **autograd** and **GPU acceleration**\n",
    "\n",
    "Both Keras and PyTorch are excellent â€” the choice often depends on the task and your preference.\n",
    "\n",
    "ğŸ§ª Try recreating this network for other datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Congratulations â€“ You've Completed the Machine Learning Series!\n",
    "\n",
    "Over the past three sessions, you've built a strong foundation in machine learning â€“ from simple models to deep neural networks powering modern AI.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§­ Your Learning Journey\n",
    "\n",
    "**ğŸ”¹ Class 1: Linear Models**\n",
    "- Discovered how machines learn from data with straight-line models  \n",
    "- Understood gradient descent as a tool for optimization  \n",
    "- Built the mathematical intuition behind model training  \n",
    "\n",
    "**ğŸ”¹ Class 2: Decision Trees & Ensembles**  \n",
    "- Learned how models split data into interpretable rules  \n",
    "- Saw how combining many trees creates powerful predictors (Random Forests)  \n",
    "- Gained tools that balance accuracy and explainability  \n",
    "\n",
    "**ğŸ”¹ Class 3: Neural Networks & Deep Learning**  \n",
    "- Explored how layered â€œneuronsâ€ can learn complex, non-linear patterns  \n",
    "- Trained deep models to recognize patterns in visuals and data  \n",
    "- Saw how AI powers modern tools like voice assistants, image recognition, and language translation  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Big Takeaways\n",
    "\n",
    "- ğŸ§  **No single model fits all problems** â€“ know your tools  \n",
    "- ğŸ” **Complex patterns require complex models**, but they also need more data and care  \n",
    "- âš–ï¸ **Simplicity vs. performance is always a trade-off**  \n",
    "- ğŸ”„ **Interpreting and validating results is as important as building the model**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒ Where Machine Learning Shows Up Around You\n",
    "\n",
    "- ğŸ¬ **Netflix recommendations** â€“ collaborative filtering + neural nets  \n",
    "- ğŸ“¸ **Face recognition** â€“ deep convolutional networks  \n",
    "- âœ‰ï¸ **Spam detection** â€“ decision trees on text features  \n",
    "- ğŸš— **Self-driving perception** â€“ computer vision + reinforcement learning  \n",
    "- ğŸŒ **Real-time translation** â€“ sequence models and transformers  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ What's Next?\n",
    "\n",
    "**ğŸ”§ Practice & Projects**\n",
    "- Apply what youâ€™ve learned to datasets from your field  \n",
    "- Build your first real-world ML mini-project  \n",
    "- Use platforms like Kaggle or Hugging Face to explore datasets and models  \n",
    "\n",
    "**ğŸ“š Keep Learning**\n",
    "- Dive into:\n",
    "  - Computer Vision\n",
    "  - Natural Language Processing\n",
    "  - Reinforcement Learning\n",
    "  - Generative AI  \n",
    "- Try TensorFlow, PyTorch, Scikit-Learn in more depth\n",
    "\n",
    "ğŸ’¼ **Start using machine learning in your field!**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ› ï¸ Final Wisdom\n",
    "\n",
    "You donâ€™t need a PhD to start using machine learning effectively.\n",
    "\n",
    "What matters most:\n",
    "- ğŸ” **Knowing which model fits the problem**\n",
    "- ğŸ§ª **Testing, validating, and improving your model**\n",
    "- ğŸ§  **Thinking critically about data and bias**\n",
    "- ğŸ“¢ **Explaining your results to others**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒŸ Youâ€™re Ready\n",
    "\n",
    "This is just the beginning â€” ML is a journey of curiosity, experimentation, and creativity.\n",
    "\n",
    "Go explore, build, ask questions, and make something amazing.  \n",
    "Weâ€™re excited to see what youâ€™ll do. ğŸš€ğŸ¤–\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDV-ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
